{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5119710c-a2b0-44d3-959e-0d0f19f033d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "# import pydot\n",
    "import os\n",
    "from matplotlib import pyplot as plt \n",
    "# import pandas as pd\n",
    "import graphviz\n",
    "import copy\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "from time import perf_counter\n",
    "from queue import Queue\n",
    "import networkx as nx\n",
    "from networkx import write_multiline_adjlist, read_multiline_adjlist\n",
    "# from networkx.drawing.nx_pydot import pydot_layout\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from threading import Thread\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# WARNING: This line is important for 3d plotting. DO NOT REMOVE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import decomposition\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f087c4-c420-4611-813a-aabaabc3f01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b6de4a-461a-4320-9867-1306f4b1653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/thlamp/tcga/python_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c4074c-e411-4e8a-99de-9ee887c7fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress(s):\n",
    "    sys.stdout.write(\"%s\" % (str(s)))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def message(s):\n",
    "    sys.stdout.write(\"%s\\n\" % (str(s)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17272b45-cbf3-4fed-9245-ad56655c4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "global FEATURE_VECTOR_FILENAME\n",
    "FEATURE_VECTOR_FILENAME = \"/home/thlamp/tcga/bladder_results/raw_data_integrated_matrix.txt\"\n",
    "\n",
    "global Prefix\n",
    "Prefix = \"\"\n",
    "\n",
    "global THREADS_TO_USE\n",
    "THREADS_TO_USE = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e805fe-a299-4d66-a562-4bbec905288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global FEATURE_VECTOR_FILENAME\n",
    "# FEATURE_VECTOR_FILENAME = \"/home/thlamp/tcga/bladder_results/normalized_data_integrated_matrix.txt\"\n",
    "# global Prefix\n",
    "# Prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3fa0d16-1efa-461f-886e-bb1bc93dcaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeFeatureMatrices(bResetFiles=False, bPostProcessing=True, bNormalize=True,\n",
    "                              bNormalizeLog2Scale=True):\n",
    "    \"\"\"\n",
    "    Initializes the case/instance feature matrices, also creating intermediate files for faster startup.\n",
    "\n",
    "    :param bResetFiles: If True, then reset/recalculate intermediate files. Default: False.\n",
    "    :param bPostProcessing: If True, then apply post-processing to remove NaNs, etc. Default: True.\n",
    "    :param bNormalize: If True, then apply normalization to the initial data. Default: True.\n",
    "    :param bNormalizeLog2Scale: If True, then apply log2 scaling after normalization to the initial data. Default: True.\n",
    "    :return: The initial feature matrix of the cases/instances.\n",
    "    \"\"\"\n",
    "\n",
    "    message(\"Opening files...\")\n",
    "\n",
    "    try:\n",
    "        if bResetFiles:\n",
    "            raise Exception(\"User requested file reset...\")\n",
    "        message(\"Trying to load saved data...\")\n",
    "\n",
    "        # Apply np.load hack\n",
    "        ###################\n",
    "        # save np.load\n",
    "        np_load_old = np.load ##return the input array from a disk file with npy extension(.npy)\n",
    "\n",
    "        # modify the default parameters of np.load\n",
    "        np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)##lambda *a, **k: orizetai to lamda function and *a, **k function parameters\n",
    "        ##np.load is a function provided by the NumPy library, typically used for loading data from saved files\n",
    "        ## BUT np.load = ... assigns a new behavior to the np.load function. This assignment changes how the np.load function works for the duration of the current scope or context in which it's defined\n",
    "        \n",
    "        ##lambda *a, **k: defines an anonymous function (a lambda function) that takes any number of positional arguments as a tuple a and any number of keyword arguments as a dictionary k. This lambda function is like a wrapper around the original np.load function.\n",
    "        ## call load_data with allow_pickle implicitly set to true\n",
    "        \n",
    "        ##allow_pickle=True: An additional keyword argument specifying that pickled objects are allowed to be loaded.\n",
    "        \n",
    "        datafile = np.load(Prefix + \"patientAndControlData.mat.npy\")\n",
    "        labelfile = np.load(Prefix + \"patientAndControlDataLabels.mat.npy\")\n",
    "\n",
    "        # restore np.load for future normal usage\n",
    "        np.load = np_load_old ##orizei to np.load sthn default, arxikh leitourgia\n",
    "        ####################\n",
    "\n",
    "        clinicalfile = loadTumorStage()##epistrefei A matrix indicating the tumor stage per case/instance\n",
    "        message(\"Trying to load saved data... Done.\")\n",
    "    except Exception as eCur:\n",
    "        message(\"Trying to load saved data... Failed:\\n%s\" % (str(eCur)))\n",
    "        message(\"Trying to load saved data from txt...\")\n",
    "        fControl = open(FEATURE_VECTOR_FILENAME, \"r\")\n",
    "        message(\"Loading labels and ids...\")\n",
    "        # labelfile, should have stored tumor_stage or labels?       \n",
    "        \n",
    "        labelfile = np.genfromtxt(fControl, skip_header=1, usecols=(0, 100472),\n",
    "                                  missing_values=['NA', \"na\", '-', '--', 'n/a'],\n",
    "                                  dtype=np.dtype(\"object\"), delimiter=' ').astype(str)\n",
    "        ##numpy.genfromtxt function to read data from a file. This function is commonly used to load data from text files into a NumPy array.\n",
    "        ##dtype=np.dtype(\"object\"): This sets the data type for the resulting NumPy array to \"object,\" which is a generic data type that can hold any type of data\n",
    "        \n",
    "        #+ removes \" from first column \n",
    "        labelfile[:, 0] = np.char.replace(labelfile[:, 0], '\"', '')\n",
    "\n",
    "        fControl.close()\n",
    "        \n",
    "        message(\"This is the label file...\")\n",
    "        message(labelfile)\n",
    "        \n",
    "        message(\"Splitting features, this is the size of labelfile\")\n",
    "        message(np.shape(labelfile))\n",
    "\n",
    "        message(\"Loading labels and ids... Done.\")\n",
    "        #---substitute loadTumorStage with clinicalfile = np.genfromtxt ...\n",
    "        #clinicalfile = loadTumorStage()\n",
    "        # Reset the file cursor to the beginning\n",
    "        \n",
    "        clinicalfile = loadTumorStage()\n",
    "        \n",
    "        \n",
    "\n",
    "        datafile = loadPatientAndControlData()##return: the patient and control feature data file as a matrix\n",
    "        message(\"Trying to load saved data from txt... Done.\")\n",
    "\n",
    "        # Saving\n",
    "        saveLoadedData(datafile, labelfile)##Saves intermediate data and label file matrices for quick loading.\n",
    "\n",
    "    message(\"Opening files... Done.\")\n",
    "\t\n",
    "    # Split feature set to features/target field\n",
    "    mFeatures, vClass, sampleIDs = splitFeatures(clinicalfile, datafile, labelfile)##return: A tuple of the form (matrix of features, matrix of labels, sample ids)\n",
    "    \n",
    "    mControlFeatureMatrix = getControlFeatureMatrix(mFeatures, vClass)#return: The subset of the data matrix, reflecting only control cases/instances.\n",
    "    message(\"1 .This is the shape of the control matrix:\")\n",
    "    message(np.shape(mControlFeatureMatrix))\n",
    "\n",
    "\n",
    "    # the new bPostProcessing removes columns from mFeatures and mControlFeatureMatrix\n",
    "    if bPostProcessing:\n",
    "        # mFeatures = postProcessFeatures(mFeatures, mControlFeatureMatrix)\n",
    "        ## for the updated postProcessFeatures\n",
    "        mFeatures, sampleIDs, vClass, feat_names = postProcessFeatures(mFeatures, vClass, sampleIDs)\n",
    "        ##return: The post-processed matrix, without NaNs.\n",
    "\n",
    "    # Update control matrix, taking into account postprocessed data\n",
    "    mControlFeatureMatrix = getControlFeatureMatrix(mFeatures, vClass)\n",
    "    ##Update control matrix, taking into account postprocessed data, an exei tre3ei to bPostProcessing dn 8a exei NaNs\n",
    "\n",
    "    message(\"2 .This is the shape of the control matrix:\")\n",
    "    message(np.shape(mControlFeatureMatrix))\n",
    "\n",
    "    if bNormalize:\n",
    "        mFeatures = normalizeDataByControl(mFeatures, mControlFeatureMatrix, bNormalizeLog2Scale)\n",
    "        ##return: The normalized and - possibly - log scaled version of the input feature matrix.\n",
    "\n",
    "    # return feat_names in the function with updated postProcessFeatures\n",
    "    return mFeatures, vClass, sampleIDs, feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c1cab8a-2c93-49a2-9abd-0e0556470fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTumorStage():\n",
    "    \"\"\"\n",
    "    Gets tumor stage data from clinical data file.\n",
    "    :return: A matrix indicating the tumor stage per case/instance.\n",
    "    \"\"\"\n",
    "    # Tumor stage\n",
    "    message(\"Loading tumor stage...\")\n",
    "    fControl = open(FEATURE_VECTOR_FILENAME, \"r\")\n",
    "    # While loading stage, also convert string to integer\n",
    "    clinicalfile = np.genfromtxt(fControl, skip_header=1, usecols=(0, 100473),\n",
    "                                  missing_values=['NA', \"na\", '-', '--', 'n/a'],\n",
    "                                  dtype=np.dtype(\"object\"), delimiter=' ').astype(str)\n",
    "    ##np.genfromtxt: This is a NumPy function used to load data from text files, including delimited files. It's a flexible function that can handle a variety of file formats.\n",
    "    ##dtype=np.dtype(\"object\"): This specifies the data type for the loaded data. The data type is set to \"object,\" which is a flexible data type that can hold a variety of data types, including strings.\n",
    "    clinicalfile[:, 0] = np.char.replace(clinicalfile[:, 0], '\"', '')\n",
    "    fControl.close()\n",
    "    message(\"Loading tumor stage... Done.\")\n",
    "    message(\"This is the clinical file...\")\n",
    "    message(clinicalfile)\n",
    "    message(\"These are the dimensions of the clinical file\")\n",
    "    message(np.shape(clinicalfile))\n",
    "    return clinicalfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a689f9-6e2b-42db-b10d-09a50ff3103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getFeatureNames():\n",
    "#     \"\"\"\n",
    "#     :return: The list of feature names.\n",
    "#     \"\"\"\n",
    "#     message(\"Loading feature names...\")\n",
    "#     fControl = open(FEATURE_VECTOR_FILENAME, \"r\")\n",
    "#     # Assuming fMatrix is the file path\n",
    "#     df = pd.read_csv(fControl, delimiter=' ')\n",
    "    \n",
    "#     # Extract the column names\n",
    "#     column_names = df.columns\n",
    "\n",
    "#     ## keep the column names except last two with tumor stage and labels\n",
    "#     column_names = column_names[:-2]\n",
    "#     print(column_names)\n",
    "#     ##numpy.genfromtxt function to read data from a file. This function is commonly used to load data from text files into a NumPy array.\n",
    "#     ##dtype=np.dtype(\"float\"): This sets the data type for the resulting NumPy array to float\n",
    "#     fControl.close()\n",
    "#     message(\"Loading feature names... Done.\")\n",
    "\n",
    "#     return column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "793550af-3d6d-4eab-ae1c-fd6672085752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureNames():\n",
    "    \"\"\"\n",
    "    :return: The list of feature names.\n",
    "    \"\"\"\n",
    "    message(\"Loading feature names...\")\n",
    "    # Read the first line from the file\n",
    "    with open(FEATURE_VECTOR_FILENAME, 'r') as file:\n",
    "        first_line = file.readline()\n",
    "    \n",
    "    # Separate the contents by space and store them in a list\n",
    "    column_names = first_line.split()\n",
    "    \n",
    "    #Remove label and tumor stage\n",
    "    column_names = column_names[:-2]\n",
    "    \n",
    "    # Remove double quotes from all elements in the list\n",
    "    column_names = [element.replace('\"', '') for element in column_names]\n",
    "    message(\"Loading feature names... Done.\")\n",
    "    return column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7838ef19-d5b1-4e97-b6e5-9e0abd6b1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPatientAndControlData():\n",
    "    \"\"\"\n",
    "    Loads and returns the serialized patient and control feature data file as a matrix.\n",
    "    :return: the patient and control feature data file as a matrix\n",
    "    \"\"\"\n",
    "    message(\"Loading features...\")\n",
    "    fControl = open(FEATURE_VECTOR_FILENAME, \"r\")\n",
    "    datafile = np.genfromtxt(fControl, skip_header=1, usecols=range(1, 100472),\n",
    "                             missing_values=['NA', \"na\", '-', '--', 'n/a'], delimiter=\" \",\n",
    "                             dtype=np.dtype(\"float\")\n",
    "                             )\n",
    "    ##numpy.genfromtxt function to read data from a file. This function is commonly used to load data from text files into a NumPy array.\n",
    "    ##dtype=np.dtype(\"float\"): This sets the data type for the resulting NumPy array to float\n",
    "    fControl.close()\n",
    "\n",
    "    message(\"This is the datafile...\")\n",
    "    message(datafile)\n",
    "    message(\"Loading features... Done.\")\n",
    "    return datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52241c98-8867-4d19-b928-a4230116529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveLoadedData(datafile, labelfile):\n",
    "    \"\"\"\n",
    "    Saves intermediate data and label file matrices for quick loading.\n",
    "    :param datafile: The matrix containing the feature data.\n",
    "    :param labelfile: The matrix containing the label data.\n",
    "    \"\"\"\n",
    "    message(\"Saving data in dir...\" + os.getcwd())\n",
    "    np.save(Prefix + \"patientAndControlData.mat.npy\", datafile)\n",
    "    np.save(Prefix + \"patientAndControlDataLabels.mat.npy\", labelfile)\n",
    "    message(\"Saving data... Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2770140-9113-435e-b077-07e74e8597a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitFeatures(clinicalfile, datafile, labelfile): \n",
    "    \"\"\"\n",
    "    Extracts class and instance info, returning them as separate matrices, where rows correspond to the same\n",
    "    case/instance.\n",
    "\n",
    "    :param clinicalfile: The file with the clinical info.\n",
    "    :param datafile: The matrix containing the full feature data from the corresponding file.\n",
    "    :param labelfile: The matrix containing  the full label data from the corresponding file.\n",
    "    :return: A tuple of the form (matrix of features, matrix of labels)\n",
    "    Chris update: :return: A tuple of the form (matrix of features, matrix of labels, sample ids)\n",
    "    \"\"\"\n",
    "    message(\"Splitting features...\")\n",
    "    message(\"Number of features: %d\"%(np.size(datafile, 1)))\n",
    "    message(\"This is the label file:\")\n",
    "    message(labelfile)\n",
    "    message(\"This is the shape of the labelfile: %s\" % (str(np.shape(labelfile))))\n",
    "    mFeatures = datafile[:, :]##datafile = the patient and control feature data file as a matrix\n",
    "    # Create matrix with extra column (to add tumor stage)\n",
    "\n",
    "    # iFeatCount = np.shape(mFeaturesOnly)[1] + 1\n",
    "    \n",
    "    ##np.shape(mFeaturesOnly)[1] After getting the shape, you are accessing the second element of the tuple, which corresponds to the number of columns in the array\n",
    "    ##number of columns + 1\n",
    "    # DEBUG LINES\n",
    "    message(\"Label file rows: %d\\tFeature file rows: %d\"%(np.shape(labelfile)[0], np.shape(mFeatures)[0]))\n",
    "    #############\n",
    "\n",
    "    # mFeatures = np.zeros((np.shape(mFeaturesOnly)[0], iFeatCount))\n",
    "    # mFeatures[:, :-1] = mFeaturesOnly\n",
    "    # mFeatures[:, iFeatCount - 1] = np.nan##last column of the NumPy array mFeatures to np.nan\n",
    "    \n",
    "    # #---\n",
    "    # #tumorStageToInt = np.vectorize(convertTumorType)##Converts tumor stages to float numbers, based on an index of classes.\n",
    "    # choicelist = clinicalfile[:, 1].astype(float)\n",
    "    # ## clinicalfile[:, 1]: This code extracts the entire second column (column with index 1) from the NumPy array clinicalfile\n",
    "    # ## choicelist contains the float number representations of tumor stages from the second column of clinicalfile    \n",
    "\n",
    "    # ## replace 0 (missing values) in tumor stage with nan\n",
    "    # choicelist = np.where(choicelist==0, np.nan, choicelist)\n",
    "\n",
    "    # # For every row\n",
    "    # for iCnt in range(np.shape(labelfile)[0]):\n",
    "    #     condlist = clinicalfile[:, 0] == labelfile[iCnt, 0]##comparing the elements and storing the result in the condlist will be a Boolean array with True, false\n",
    "\n",
    "    #     ## clinicalfile[:, 1]: This code extracts the entire second column (column with index 1) from the NumPy array clinicalfile\n",
    "    #     ## choicelist contains the float number representations of tumor stages from the second column of clinicalfile\n",
    "    #     # Update the last feature, by joining on ID\n",
    "    #     mFeatures[iCnt, iFeatCount - 1] = np.select(condlist, choicelist)\n",
    "        ##mFeatures[iCnt, iFeatCount - 1] iCnt is used as the row index and last column\n",
    "        ##np.select will select values from choicelist based on the corresponding conditions in condlist\n",
    "    vClass = labelfile[:, 1]\n",
    "    sampleIDs = labelfile[:, 0]\n",
    "    print(\"This is the vClass: \")\n",
    "    print(vClass)\n",
    "    # DEBUG LINES\n",
    "    message(\"Found classes:\\n%s\" % (str(vClass)))\n",
    "    message(\"Found sample IDs:\\n%s\" % (str(sampleIDs)))\n",
    "    #############\n",
    "    # DEBUG LINES\n",
    "    # message(\"Found tumor types:\\n%s\" % (\n",
    "    #     \"\\n\".join([\"%s:%s\" % (x, y) for x, y in zip(labelfile[:, 0], mFeatures[:, iFeatCount - 1])])))\n",
    "    #############\n",
    "    message(\"Splitfeatures: This is the mFeatures...\")\n",
    "    message(mFeatures)\n",
    "    message(\"Splitting features... Done.\")\n",
    "\n",
    "    return mFeatures, vClass, sampleIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc3f05fb-8fd6-44b6-8475-50a83e1143db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def splitFeatures(clinicalfile, datafile, labelfile): \n",
    "#     \"\"\"\n",
    "#     Extracts class and instance info, returning them as separate matrices, where rows correspond to the same\n",
    "#     case/instance.\n",
    "\n",
    "#     :param clinicalfile: The file with the clinical info.\n",
    "#     :param datafile: The matrix containing the full feature data from the corresponding file.\n",
    "#     :param labelfile: The matrix containing  the full label data from the corresponding file.\n",
    "#     :return: A tuple of the form (matrix of features, matrix of labels)\n",
    "#     Chris update: :return: A tuple of the form (matrix of features, matrix of labels, sample ids)\n",
    "#     \"\"\"\n",
    "#     message(\"Splitting features...\")\n",
    "#     message(\"Number of features: %d\"%(np.size(datafile, 1)))\n",
    "#     message(\"This is the label file:\")\n",
    "#     message(labelfile)\n",
    "#     message(\"This is the shape of the labelfile: %s\" % (str(np.shape(labelfile))))\n",
    "#     mFeaturesOnly = datafile[:, :]##datafile = the patient and control feature data file as a matrix\n",
    "#     # Create matrix with extra column (to add tumor stage)\n",
    "\n",
    "#     iFeatCount = np.shape(mFeaturesOnly)[1] + 1\n",
    "    \n",
    "#     ##np.shape(mFeaturesOnly)[1] After getting the shape, you are accessing the second element of the tuple, which corresponds to the number of columns in the array\n",
    "#     ##number of columns + 1\n",
    "#     # DEBUG LINES\n",
    "#     message(\"Label file rows: %d\\tFeature file rows: %d\"%(np.shape(labelfile)[0], np.shape(mFeaturesOnly)[0]))\n",
    "#     #############\n",
    "\n",
    "#     mFeatures = np.zeros((np.shape(mFeaturesOnly)[0], iFeatCount))\n",
    "#     mFeatures[:, :-1] = mFeaturesOnly\n",
    "#     mFeatures[:, iFeatCount - 1] = np.nan##last column of the NumPy array mFeatures to np.nan\n",
    "    \n",
    "#     #---\n",
    "#     #tumorStageToInt = np.vectorize(convertTumorType)##Converts tumor stages to float numbers, based on an index of classes.\n",
    "#     choicelist = clinicalfile[:, 1].astype(float)\n",
    "#     ## clinicalfile[:, 1]: This code extracts the entire second column (column with index 1) from the NumPy array clinicalfile\n",
    "#     ## choicelist contains the float number representations of tumor stages from the second column of clinicalfile    \n",
    "\n",
    "#     ## replace 0 (missing values) in tumor stage with nan\n",
    "#     choicelist = np.where(choicelist==0, np.nan, choicelist)\n",
    "\n",
    "#     # For every row\n",
    "#     for iCnt in range(np.shape(labelfile)[0]):\n",
    "#         condlist = clinicalfile[:, 0] == labelfile[iCnt, 0]##comparing the elements and storing the result in the condlist will be a Boolean array with True, false\n",
    "\n",
    "#         ## clinicalfile[:, 1]: This code extracts the entire second column (column with index 1) from the NumPy array clinicalfile\n",
    "#         ## choicelist contains the float number representations of tumor stages from the second column of clinicalfile\n",
    "#         # Update the last feature, by joining on ID\n",
    "#         mFeatures[iCnt, iFeatCount - 1] = np.select(condlist, choicelist)\n",
    "#         ##mFeatures[iCnt, iFeatCount - 1] iCnt is used as the row index and last column\n",
    "#         ##np.select will select values from choicelist based on the corresponding conditions in condlist\n",
    "#     vClass = labelfile[:, 1]\n",
    "#     sampleIDs = labelfile[:, 0]\n",
    "#     print(\"This is the vClass: \")\n",
    "#     print(vClass)\n",
    "#     # DEBUG LINES\n",
    "#     message(\"Found classes:\\n%s\" % (str(vClass)))\n",
    "#     message(\"Found sample IDs:\\n%s\" % (str(sampleIDs)))\n",
    "#     #############\n",
    "#     # DEBUG LINES\n",
    "#     # message(\"Found tumor types:\\n%s\" % (\n",
    "#     #     \"\\n\".join([\"%s:%s\" % (x, y) for x, y in zip(labelfile[:, 0], mFeatures[:, iFeatCount - 1])])))\n",
    "#     #############\n",
    "#     message(\"Splitfeatures: This is the mFeatures...\")\n",
    "#     message(mFeatures)\n",
    "#     message(\"Splitting features... Done.\")\n",
    "\n",
    "#     return mFeatures, vClass, sampleIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5654b044-0040-4c6a-b383-d11ea0096dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find only the control samples\n",
    "def getControlFeatureMatrix(mAllData, vLabels):\n",
    "    \"\"\"\n",
    "    Gets the features of control samples only.\n",
    "    :param mAllData: The full matrix of data (control plus tumor data).\n",
    "    :param vLabels: The matrix of labels per case/instance.\n",
    "    :return: The subset of the data matrix, reflecting only control cases/instances.\n",
    "    \"\"\"\n",
    "    message(\"Finding only the control data...\")\n",
    "    choicelist = mAllData\n",
    "    #--- condlist = isEqualToString(vLabels, 'Solid_Tissue_Normal')##epistrefei true, false se numpy array\n",
    "    # 0 is the label for controls\n",
    "    condlist = vLabels == \"0\"\n",
    "    message(\"This is the control feature matrix:\")\n",
    "    print(choicelist[condlist])## ektupvnei osa einai true dld osa exoyn Solid_Tissue_Normal\n",
    "    message(\"Data shape: %s\" % (str(np.shape(choicelist[condlist]))))## epistrefei shape choicelist matrix\n",
    "    message(\"Finding only the control data...Done\")\n",
    "    return choicelist[condlist]##epistrefei subset tou sunolikou matrix me mono osa exoun Solid_Tissue_Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3736a2c6-8372-4f1a-91e8-eea0d6e283e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNonControlFeatureMatrix(mAllData, vLabels):\n",
    "    \"\"\"\n",
    "    Returns the subset of the feature matrix, corresponding to non-control (i.e. tumor) data.\n",
    "    :param mAllData: The full feature matrix of case/instance data.\n",
    "    :param vLabels: The label matrix, defining what instance is what type (control/tumor).\n",
    "    :return: The subset of the feature matrix, corresponding to non-control (i.e. tumor) data\n",
    "    \"\"\"\n",
    "    choicelist = mAllData\n",
    "    condlist = vLabels == \"1\"\n",
    "    return choicelist[condlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "214be30b-ef73-44bb-9f60-0009206cf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rows_with_only_NaN(mAllData, level_indices):\n",
    "#     \"\"\"\n",
    "#     :param mAllData: The full feature matrix of case/instance data.\n",
    "#     :param level_indices: The columns of the omic level to search\n",
    "#     :return: The indices of the rows that don't have data at least in one level\n",
    "#     \"\"\"\n",
    "#     # create empty array\n",
    "#     indices_of_empty_rows = np.empty(0)\n",
    "    \n",
    "#     for omic_level in level_indices:\n",
    "#         # Create a boolean mask indicating NaN values\n",
    "#         nan_mask = np.isnan(mAllData[:, omic_level[0]:omic_level[1]])\n",
    "    \n",
    "#         # Use np.all along axis 1 to check if all values in each row are True (indicating NaN)\n",
    "#         rows_with_nan = np.all(nan_mask, axis=1)\n",
    "    \n",
    "#         # Get the indices of rows with NaN\n",
    "#         indices_of_rows_with_nan = np.where(rows_with_nan)[0]\n",
    "        \n",
    "#         indices_of_empty_rows = np.append(indices_of_empty_rows, indices_of_rows_with_nan)\n",
    "\n",
    "#     indices_of_empty_rows = np.unique(indices_of_empty_rows).astype(int)\n",
    "#     return indices_of_empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "276092e5-27be-4dc0-82d8-2722d8e296fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_incomplete_samples(mAllData, sample_ids, vClass):\n",
    "#     \"\"\"\n",
    "#     :param mAllData: The full feature matrix of case/instance data.\n",
    "#     :param sample_ids: A list with sample ids\n",
    "#     :return: The feature matrix without incomplete samples and the remaining sample ids\n",
    "#     \"\"\"\n",
    "    \n",
    "#     message(\"Removing incomplete samples...\")\n",
    "#     omic_levels = omic_level_indices()\n",
    "\n",
    "#     indices_to_remove = rows_with_only_NaN(mAllData, omic_levels)\n",
    "    \n",
    "#     # Create a boolean mask to identify rows to keep\n",
    "#     mask = np.ones(mAllData.shape[0], dtype=bool)\n",
    "#     mask[indices_to_remove] = False\n",
    "    \n",
    "#     # Use boolean indexing to get the resulting matrix without specified rows\n",
    "#     mFeatures = mAllData[mask]\n",
    "    \n",
    "#     removed_sample_ids = np.take(sample_ids, indices_to_remove)\n",
    "#     message(\"The sample ids that were remored are:\"+str(removed_sample_ids))\n",
    "    \n",
    "#     updated_sample_ids = np.delete(sample_ids, indices_to_remove)\n",
    "#     updated_vClass = np.delete(vClass, indices_to_remove)\n",
    "    \n",
    "#     message(\"Removing incomplete samples... Done\")\n",
    "#     return mFeatures, updated_sample_ids, updated_vClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a025ef8f-246f-4993-a83c-c58f60d8ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rows_filtering(input_matrix, sample_ids, vClass, nan_threshold=0.2):\n",
    "#     \"\"\"\n",
    "#     It returns the filtered matrix for rows and an array with the index of the rows that were kept\n",
    "#     \"\"\"\n",
    "#     message(\"Rows' filtering... Done\")\n",
    "#     ## length of row\n",
    "#     rows_length = input_matrix.shape[1]\n",
    "#     ## count nan per row\n",
    "#     nan_per_row = count_nan_per_row(input_matrix)\n",
    "#     ## compute the frequency of nan per row\n",
    "#     nan_frequency  = nan_per_row / rows_length\n",
    "#     ## return an array with boolean values, that show the rows with <=nan_threshold \n",
    "#     rows_to_keep = nan_frequency <= nan_threshold\n",
    "\n",
    "#     # keep the sample_ids of the remaining rows \n",
    "#     updated_sample_ids = np.array(sample_ids)[rows_to_keep]\n",
    "\n",
    "#     # keep the sample_ids of the remaining rows \n",
    "#     updated_vClass = np.array(vClass)[rows_to_keep]\n",
    "    \n",
    "#     ## filtering the matrix by rows_to_keep\n",
    "#     input_matrix = input_matrix[rows_to_keep, :]\n",
    "#     message(\"The size of the new matrix is:\"+str(np.shape(input_matrix)))\n",
    "#     message(\"Rows' filtering... Done\")\n",
    "#     return input_matrix, updated_sample_ids, updated_vClass\n",
    "\n",
    "# def count_nan_per_row(input_matrix):\n",
    "#     nan_count_per_column = np.sum(np.isnan(input_matrix), axis=1)\n",
    "#     return nan_count_per_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b64d5a-2375-4563-b359-cd8e77c1ce4c",
   "metadata": {},
   "source": [
    "## NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8930a7ac-59df-4851-a7c9-ebdf3336f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckRowsNaN(input_matrix, nan_threshold=0.2):\n",
    "    \"\"\"\n",
    "    It returns the filtered matrix for rows and an array with the index of the rows that were kept\n",
    "    \"\"\"\n",
    "    message(\"Rows' filtering... Done\")\n",
    "    ## length of row\n",
    "    rows_length = input_matrix.shape[1]\n",
    "    ## count nan per row\n",
    "    nan_per_row = count_nan_per_row(input_matrix)\n",
    "    ## compute the frequency of nan per row\n",
    "    nan_frequency  = nan_per_row / rows_length\n",
    "    ## return an array with boolean values, that show the rows with <=nan_threshold \n",
    "    rows_to_remove = nan_frequency > nan_threshold\n",
    "\n",
    "    rows_to_remove = np.where(rows_to_remove)\n",
    "    # Flatten the 2D array into a 1D array \n",
    "    rows_to_remove = np.ravel(rows_to_remove)\n",
    "    # keep the sample_ids of the remaining rows \n",
    "    # updated_sample_ids = np.array(sample_ids)[rows_to_keep]\n",
    "\n",
    "    # keep the sample_ids of the remaining rows \n",
    "    # updated_vClass = np.array(vClass)[rows_to_keep]\n",
    "    \n",
    "    ## filtering the matrix by rows_to_keep\n",
    "    # input_matrix = input_matrix[rows_to_keep, :]\n",
    "    # message(\"The size of the new matrix is:\"+str(np.shape(input_matrix)))\n",
    "    # message(\"Rows' filtering... Done\")\n",
    "    return rows_to_remove\n",
    "\n",
    "def count_nan_per_row(input_matrix):\n",
    "    nan_count_per_column = np.sum(np.isnan(input_matrix), axis=1)\n",
    "    return nan_count_per_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4ab30-e7cf-4731-b1d1-17ead8f1beb3",
   "metadata": {},
   "source": [
    "## NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf4f33f3-bb6e-4164-8c9f-7161dcf93e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_with_only_NaN(mAllData, level_indices):\n",
    "    \"\"\"\n",
    "    :param mAllData: The full feature matrix of case/instance data.\n",
    "    :param level_indices: The columns of the omic level to search\n",
    "    :return: The indices of the rows that don't have data at least in one level\n",
    "    \"\"\"\n",
    "    # create empty array\n",
    "    indices_of_empty_rows = np.empty(0)\n",
    "    \n",
    "    for omic_level in level_indices:\n",
    "        # Create a boolean mask indicating NaN values\n",
    "        nan_mask = np.isnan(mAllData[:, omic_level[0]:omic_level[1]])\n",
    "    \n",
    "        # Use np.all along axis 1 to check if all values in each row are True (indicating NaN)\n",
    "        rows_with_nan = np.all(nan_mask, axis=1)\n",
    "    \n",
    "        # Get the indices of rows with NaN\n",
    "        indices_of_rows_with_nan = np.where(rows_with_nan)[0]\n",
    "        \n",
    "        indices_of_empty_rows = np.append(indices_of_empty_rows, indices_of_rows_with_nan)\n",
    "\n",
    "    indices_of_empty_rows = np.unique(indices_of_empty_rows).astype(int)\n",
    "    return indices_of_empty_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70a608-5ef8-47c3-97d9-6936bc75e91d",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1aa4583-0db8-425b-8d74-3912928753ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckColsNaN(input_matrix,nan_threshold=0.2):\n",
    "    \"\"\"\n",
    "    It returns the filtered matrix for columns and an array with the index of the columns that were kept\n",
    "    \"\"\"\n",
    "    message(\"Columns' filtering... \")\n",
    "    ## length of columns\n",
    "    columns_length = input_matrix.shape[0]\n",
    "    ## count nan per column\n",
    "    nan_per_column = count_nan_per_column(input_matrix)\n",
    "    ## compute the frequency of nan per column\n",
    "    nan_frequency  = nan_per_column / columns_length\n",
    "    ## return an array with boolean values, that show the columns with <=nan_threshold t\n",
    "    columns_to_remove = nan_frequency > nan_threshold\n",
    "\n",
    "    columns_to_remove = np.where(columns_to_remove)\n",
    "    # Flatten the 2D array into a 1D array \n",
    "    columns_to_remove = np.ravel(columns_to_remove)\n",
    "    # features = getFeatureNames()\n",
    "\n",
    "    # # keep the feature names of the remaining columns \n",
    "    # updated_feature_names = np.array(features)[columns_to_keep]\n",
    "    \n",
    "    # ## filtering the matrix by columns_to_keep\n",
    "    # input_matrix = input_matrix[:, columns_to_keep]\n",
    "    # message(\"The size of the new matrix is:\"+str(np.shape(input_matrix)))\n",
    "    # message(\"Columns' filtering... Done\")\n",
    "    return columns_to_remove\n",
    "\n",
    "def count_nan_per_column(input_matrix):\n",
    "    nan_count_per_column = np.sum(np.isnan(input_matrix), axis=0)\n",
    "    return nan_count_per_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b1bcd-3a04-4969-84a4-63ca89a8aa3a",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fc75743-0be6-45e2-ae79-0b94a016a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postProcessFeatures(mFeatures, vClass, sample_ids):\n",
    "    \"\"\"\n",
    "    Post-processes feature matrix to replace NaNs with control instance feature mean values, and also to remove\n",
    "    all-NaN columns.\n",
    "\n",
    "    :param mFeatures: The matrix to pre-process.\n",
    "    :param mControlFeatures: The subset of the input matrix that reflects control instances.\n",
    "    :param sample_ids: A list with sample ids.\n",
    "    :return: The post-processed matrix, without NaNs.\n",
    "    \"\"\"\n",
    "    message(\"Replacing NaNs from feature set...\")\n",
    "    # DEBUG LINES\n",
    "    message(\"Data shape before replacement: %s\" % (str(np.shape(mFeatures))))\n",
    "    #############\n",
    "\n",
    "    # WARNING: Imputer also throws away columns it does not like\n",
    "    # imputer = Imputer(strategy=\"mean\", missing_values=\"NaN\", verbose=1)\n",
    "    # mFeatures_noNaNs = imputer.fit_transform(mFeatures)\n",
    "\n",
    "    rows_to_remove = CheckRowsNaN(mFeatures)\n",
    "\n",
    "    levels_indices = omic_level_indices()\n",
    "    \n",
    "    incomplete_samples = rows_with_only_NaN(mFeatures, levels_indices)\n",
    "    \n",
    "    samples_to_remove = np.concatenate((rows_to_remove, incomplete_samples))\n",
    "    samples_to_remove = np.unique(samples_to_remove)\n",
    "    \n",
    "    features_to_remove = CheckColsNaN(mFeatures)\n",
    "    \n",
    "    # Remove samples from the matrix\n",
    "    mFeatures = np.delete(mFeatures, samples_to_remove, axis=0)\n",
    "\n",
    "    # Remove features from the matrix\n",
    "    mFeatures = np.delete(mFeatures, features_to_remove, axis=1)\n",
    "\n",
    "    # Create a boolean mask to keep elements not in the indices_to_remove array\n",
    "    mask = np.ones(len(sample_ids), dtype=bool)\n",
    "    mask[samples_to_remove] = False\n",
    "\n",
    "    message(\"mask:\"+str(np.shape(mask)))\n",
    "    message(\"vClass:\"+str(np.shape(vClass)))\n",
    "    # Use the mask to filter the array\n",
    "    filtered_sample_ids = sample_ids[mask]\n",
    "    filtered_vClass = vClass[mask]\n",
    "\n",
    "    features = getFeatureNames()\n",
    "    # Create a new list without the elements at the specified indices\n",
    "    filtered_features = [element for index, element in enumerate(features) if index not in features_to_remove]\n",
    "    \n",
    "    # mFeatures, sample_ids, filtered_vClass = remove_incomplete_samples(mFeatures, sample_ids, vClass)\n",
    "\n",
    "    # mFeatures, filtered_feature_names = columns_filtering(mFeatures)\n",
    "\n",
    "    # mFeatures, filtered_sample_ids, filtered_vClass = rows_filtering(input_matrix, sample_ids, filtered_vClass)\n",
    "    message(mFeatures)\n",
    "    mControlFeatures = getControlFeatureMatrix(mFeatures, filtered_vClass)\n",
    "    \n",
    "    # Extract means per control col\n",
    "    # :-1 beccause the last column is about tunor stage\n",
    "    mMeans = np.nanmean(mControlFeatures[:, :], axis=0)##calculate the mean of an array mControlFeatures along a specific axis\n",
    "    # Find nans\n",
    "    inds = np.where(np.isnan(mFeatures[:, :]))## Find nans\n",
    "    # Do replacement\n",
    "    mFeatures[inds] = np.take(mMeans, inds[1])## Do replacement\n",
    "\n",
    "    message(\"Are there any NaNs after postProcessing?\")\n",
    "    message(np.any(np.isnan(mFeatures[:, :])))\n",
    "    # DEBUG LINES\n",
    "    message(\"Data shape after replacement: %s\" % (str(np.shape(mFeatures))))\n",
    "    #############\n",
    "\n",
    "    # TODO: Check below\n",
    "    # WARNING: If a control data feature was fully NaN, but the corresponding case data had only SOME NaN,\n",
    "    # we would NOT successfully deal with the case data NaN, because there would be no mean to replace them by.\n",
    "\n",
    "    #############\n",
    "    message(\"Replacing NaNs from feature set... Done.\")\n",
    "\n",
    "    # Convert np array to panda dataframe\n",
    "    arr = np.array(mFeatures)\n",
    "\n",
    "    message(\"Removing features that have only NaN values...\")\n",
    "    # mask = np.all(np.isnan(mFeatures), axis=0)\n",
    "\t##mask variable will be a boolean array with True at the positions where all values in the corresponding columns of mFeatures are NaN, and False where there is at least one non-NaN value in that column\n",
    "    # mFeatures = mFeatures[:, ~mask]\n",
    "    ##remove columns from the mFeatures array where all values are NaN, based on the mask boolean array\n",
    "    message(\"Number of features after removal: %s\" % (str(np.shape(mFeatures))))\n",
    "    # message(mFeatures)\n",
    "    # message(\"Removing features that have only NaN values...Done\")\n",
    "\n",
    "    message(\"Are there any NaNs after postProcessing, except last column?\")\n",
    "    message(np.any(np.isnan(mFeatures[:, :])))\n",
    "\n",
    "    message(\"This is mFeatures in postProcessing...\")\n",
    "    message(mFeatures)\n",
    "    return mFeatures, filtered_sample_ids, filtered_vClass, filtered_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fad66e7a-4467-4b4d-8da3-3ae8650f76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def columns_filtering(input_matrix,nan_threshold=0.2):\n",
    "#     \"\"\"\n",
    "#     It returns the filtered matrix for columns and an array with the index of the columns that were kept\n",
    "#     \"\"\"\n",
    "#     message(\"Columns' filtering... \")\n",
    "#     ## length of columns\n",
    "#     columns_length = input_matrix.shape[0]\n",
    "#     ## count nan per column\n",
    "#     nan_per_column = count_nan_per_column(input_matrix)\n",
    "#     ## compute the frequency of nan per column\n",
    "#     nan_frequency  = nan_per_column / columns_length\n",
    "#     ## return an array with boolean values, that show the columns with <=nan_threshold t\n",
    "#     columns_to_keep = nan_frequency <= nan_threshold\n",
    "    \n",
    "#     features = getFeatureNames()\n",
    "\n",
    "#     # keep the feature names of the remaining columns \n",
    "#     updated_feature_names = np.array(features)[columns_to_keep]\n",
    "    \n",
    "#     ## filtering the matrix by columns_to_keep\n",
    "#     input_matrix = input_matrix[:, columns_to_keep]\n",
    "#     message(\"The size of the new matrix is:\"+str(np.shape(input_matrix)))\n",
    "#     message(\"Columns' filtering... Done\")\n",
    "#     return input_matrix,updated_feature_names\n",
    "\n",
    "# def count_nan_per_column(input_matrix):\n",
    "#     nan_count_per_column = np.sum(np.isnan(input_matrix), axis=0)\n",
    "#     return nan_count_per_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f00dc65-f7d7-46b6-8b1b-9be3987569fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mRNA \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m[:, :\u001b[38;5;241m60660\u001b[39m]\n\u001b[1;32m      2\u001b[0m miRNA \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m60660\u001b[39m:\u001b[38;5;241m62541\u001b[39m]\n\u001b[1;32m      3\u001b[0m methylation \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m62541\u001b[39m:]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "mRNA = data[:, :60660]\n",
    "miRNA = data[:, 60660:62541]\n",
    "methylation = data[:, 62541:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10faef5-ee20-4984-a1dd-0d821addd621",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_for_cols(miRNA,\"miRNA\")\n",
    "hist_for_rows(miRNA,\"miRNA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3a2815a-3a19-4321-abc1-766269858ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_for_cols(data,level):\n",
    "    freq = count_nan_per_column(data)\n",
    "    # print(a)\n",
    "    # Creating plot\n",
    "    \n",
    "    relative_freq = freq/np.shape(data)[0]\n",
    "    # print(np.shape(data)[0])\n",
    "    # print(b)\n",
    "    # Creating plot\n",
    "    fig = plt.figure(figsize =(10, 7))\n",
    "    \n",
    "    # Set the bins parameter to control the width of each column\n",
    "    bins = np.arange(0, 1.1, 0.1)  # Adjust the range and step size as needed\n",
    "    \n",
    "    # Save the objects, in order to keep bars for bar_label\n",
    "    counts, edges, bars = plt.hist(relative_freq, bins=bins) \n",
    "    ## plot labels over the bars\n",
    "    plt.bar_label(bars)\n",
    "    \n",
    "    # Set x-axis limits (ensure minimum value is set to 0)\n",
    "    plt.xlim(0, 1)\n",
    "    \n",
    "    # Set x-axis ticks every 0.1\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    \n",
    "    plt.title(\"Relative frequency histogram for features in \"+level) \n",
    "    \n",
    "    plt.xlabel('Relative frequency of NaN')\n",
    "    plt.ylabel('Counts')\n",
    "    # show plot\n",
    "    plt.show()\n",
    "    # fig.savefig(level+'relative_freq.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "392e0817-8480-4c41-ba68-3a00f172dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_for_rows(data,level):\n",
    "    freq = count_nan_per_row(data)\n",
    "    # print(a)\n",
    "    \n",
    "    relative_freq = freq/np.shape(data)[1]\n",
    "    # print(np.shape(data)[0])\n",
    "    # print(b)\n",
    "    # Creating plot\n",
    "    fig = plt.figure(figsize =(10, 7))\n",
    "    \n",
    "    # Set the bins parameter to control the width of each column\n",
    "    bins = np.arange(0, 1.1, 0.1)  # Adjust the range and step size as needed\n",
    "    \n",
    "    # Save the objects, in order to keep bars for bar_label\n",
    "    counts, edges, bars = plt.hist(relative_freq, bins=bins) \n",
    "    ## plot labels over the bars\n",
    "    plt.bar_label(bars)\n",
    "    \n",
    "    # Set x-axis limits (ensure minimum value is set to 0)\n",
    "    plt.xlim(0, 1)\n",
    "    \n",
    "    # Set x-axis ticks every 0.1\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    \n",
    "    plt.title(\"Relative frequency histogram for patients in \"+level) \n",
    "\n",
    "    plt.xlabel('Relative frequency of NaN')\n",
    "    plt.ylabel('Counts')\n",
    "    # show plot\n",
    "    plt.show()\n",
    "    # fig.savefig(level+'relative_freq_in_patients.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ec223cd-9c4c-40f6-94b5-8299c6fede6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def postProcessFeatures(mFeatures, mControlFeatures):\n",
    "#     \"\"\"\n",
    "#     Post-processes feature matrix to replace NaNs with control instance feature mean values, and also to remove\n",
    "#     all-NaN columns.\n",
    "\n",
    "#     :param mFeatures: The matrix to pre-process.\n",
    "#     :param mControlFeatures: The subset of the input matrix that reflects control instances.\n",
    "#     :return: The post-processed matrix, without NaNs.\n",
    "#     \"\"\"\n",
    "#     message(\"Replacing NaNs from feature set...\")\n",
    "#     # DEBUG LINES\n",
    "#     message(\"Data shape before replacement: %s\" % (str(np.shape(mFeatures))))\n",
    "#     #############\n",
    "\n",
    "#     # WARNING: Imputer also throws away columns it does not like\n",
    "#     # imputer = Imputer(strategy=\"mean\", missing_values=\"NaN\", verbose=1)\n",
    "#     # mFeatures_noNaNs = imputer.fit_transform(mFeatures)\n",
    "\n",
    "#     # remove columns with only nan in control from all and control data \n",
    "#     mFeatures, mControlFeatures = remove_empty_cols_from_control(mFeatures, mControlFeatures)\n",
    "    \n",
    "#     # Extract means per control col\n",
    "#     # :-1 beccause the last column is about tunor stage\n",
    "#     mMeans = np.nanmean(mControlFeatures[:, :], axis=0)##calculate the mean of an array mControlFeatures along a specific axis\n",
    "#     # Find nans\n",
    "#     inds = np.where(np.isnan(mFeatures[:, :]))## Find nans\n",
    "#     # Do replacement\n",
    "#     mFeatures[inds] = np.take(mMeans, inds[1])## Do replacement\n",
    "\n",
    "#     message(\"Are there any NaNs after postProcessing?\")\n",
    "#     message(np.any(np.isnan(mFeatures[:, :])))\n",
    "#     # DEBUG LINES\n",
    "#     message(\"Data shape after replacement: %s\" % (str(np.shape(mFeatures))))\n",
    "#     #############\n",
    "\n",
    "#     # TODO: Check below\n",
    "#     # WARNING: If a control data feature was fully NaN, but the corresponding case data had only SOME NaN,\n",
    "#     # we would NOT successfully deal with the case data NaN, because there would be no mean to replace them by.\n",
    "\n",
    "#     #############\n",
    "#     message(\"Replacing NaNs from feature set... Done.\")\n",
    "\n",
    "#     # Convert np array to panda dataframe\n",
    "#     arr = np.array(mFeatures)\n",
    "\n",
    "#     message(\"Removing features that have only NaN values...\")\n",
    "#     # mask = np.all(np.isnan(mFeatures), axis=0)\n",
    "# \t##mask variable will be a boolean array with True at the positions where all values in the corresponding columns of mFeatures are NaN, and False where there is at least one non-NaN value in that column\n",
    "#     # mFeatures = mFeatures[:, ~mask]\n",
    "#     ##remove columns from the mFeatures array where all values are NaN, based on the mask boolean array\n",
    "#     message(\"Number of features after removal: %s\" % (str(np.shape(mFeatures))))\n",
    "#     # message(mFeatures)\n",
    "#     # message(\"Removing features that have only NaN values...Done\")\n",
    "\n",
    "#     message(\"Are there any NaNs after postProcessing, except last column?\")\n",
    "#     message(np.any(np.isnan(mFeatures[:, :])))\n",
    "\n",
    "#     message(\"This is mFeatures in postProcessing...\")\n",
    "#     message(mFeatures)\n",
    "#     return mFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426ec9c-4c62-4c19-9d24-9b2396cb3982",
   "metadata": {},
   "source": [
    "# !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f9af869-0489-4c3e-b22f-3947ba311d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def postProcessFeatures(mFeatures, vClass, sample_ids):\n",
    "#     \"\"\"\n",
    "#     Post-processes feature matrix to replace NaNs with control instance feature mean values, and also to remove\n",
    "#     all-NaN columns.\n",
    "\n",
    "#     :param mFeatures: The matrix to pre-process.\n",
    "#     :param mControlFeatures: The subset of the input matrix that reflects control instances.\n",
    "#     :param sample_ids: A list with sample ids.\n",
    "#     :return: The post-processed matrix, without NaNs.\n",
    "#     \"\"\"\n",
    "#     message(\"Replacing NaNs from feature set...\")\n",
    "#     # DEBUG LINES\n",
    "#     message(\"Data shape before replacement: %s\" % (str(np.shape(mFeatures))))\n",
    "#     #############\n",
    "\n",
    "#     # WARNING: Imputer also throws away columns it does not like\n",
    "#     # imputer = Imputer(strategy=\"mean\", missing_values=\"NaN\", verbose=1)\n",
    "#     # mFeatures_noNaNs = imputer.fit_transform(mFeatures)\n",
    "\n",
    "#     mFeatures, sample_ids, filtered_vClass = remove_incomplete_samples(mFeatures, sample_ids, vClass)\n",
    "\n",
    "#     mFeatures, filtered_feature_names = columns_filtering(mFeatures)\n",
    "\n",
    "#     mFeatures, filtered_sample_ids, filtered_vClass = rows_filtering(input_matrix, sample_ids, filtered_vClass)\n",
    "    \n",
    "#     mControlFeatures = getControlFeatureMatrix(mFeatures, filtered_vClass)\n",
    "    \n",
    "#     # Extract means per control col\n",
    "#     # :-1 beccause the last column is about tunor stage\n",
    "#     mMeans = np.nanmean(mControlFeatures[:, :], axis=0)##calculate the mean of an array mControlFeatures along a specific axis\n",
    "#     # Find nans\n",
    "#     inds = np.where(np.isnan(mFeatures[:, :]))## Find nans\n",
    "#     # Do replacement\n",
    "#     mFeatures[inds] = np.take(mMeans, inds[1])## Do replacement\n",
    "\n",
    "#     message(\"Are there any NaNs after postProcessing?\")\n",
    "#     message(np.any(np.isnan(mFeatures[:, :])))\n",
    "#     # DEBUG LINES\n",
    "#     message(\"Data shape after replacement: %s\" % (str(np.shape(mFeatures))))\n",
    "#     #############\n",
    "\n",
    "#     # TODO: Check below\n",
    "#     # WARNING: If a control data feature was fully NaN, but the corresponding case data had only SOME NaN,\n",
    "#     # we would NOT successfully deal with the case data NaN, because there would be no mean to replace them by.\n",
    "\n",
    "#     #############\n",
    "#     message(\"Replacing NaNs from feature set... Done.\")\n",
    "\n",
    "#     # Convert np array to panda dataframe\n",
    "#     arr = np.array(mFeatures)\n",
    "\n",
    "#     message(\"Removing features that have only NaN values...\")\n",
    "#     # mask = np.all(np.isnan(mFeatures), axis=0)\n",
    "# \t##mask variable will be a boolean array with True at the positions where all values in the corresponding columns of mFeatures are NaN, and False where there is at least one non-NaN value in that column\n",
    "#     # mFeatures = mFeatures[:, ~mask]\n",
    "#     ##remove columns from the mFeatures array where all values are NaN, based on the mask boolean array\n",
    "#     message(\"Number of features after removal: %s\" % (str(np.shape(mFeatures))))\n",
    "#     # message(mFeatures)\n",
    "#     # message(\"Removing features that have only NaN values...Done\")\n",
    "\n",
    "#     message(\"Are there any NaNs after postProcessing, except last column?\")\n",
    "#     message(np.any(np.isnan(mFeatures[:, :])))\n",
    "\n",
    "#     message(\"This is mFeatures in postProcessing...\")\n",
    "#     message(mFeatures)\n",
    "#     return mFeatures, sample_ids, filtered_feature_names, filtered_vClass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577dde6-53e1-434c-992e-273a3aa0e93d",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8da0ff4-c78e-4314-a913-13f5a409d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def omic_level_indices():\n",
    "    \"\"\"\n",
    "    Returns the columns corresponding to each omic level\n",
    "    \"\"\"\n",
    "    feature_names = getFeatureNames()\n",
    "\n",
    "    # Search for elements that start with \"ENSG\" and contain \".\"\n",
    "    indices_of_mrna = np.where(np.core.defchararray.startswith(feature_names, \"ENSG\") & (np.core.defchararray.find(feature_names, \".\") != -1))[0]\n",
    "    \n",
    "    # Search for elements that start with \"hsa\"\n",
    "    indices_of_mirna = np.where(np.core.defchararray.startswith(feature_names, \"hsa\"))[0]\n",
    "    \n",
    "    # Search for elements that start with \"ENSG\" and do not contain \".\"\n",
    "    indices_of_methylation = np.where(np.core.defchararray.startswith(feature_names, \"ENSG\") & (np.core.defchararray.find(feature_names, \".\") == -1))[0]\n",
    "\n",
    "    mrna = []\n",
    "    mirna = []\n",
    "    methylation = []\n",
    "\n",
    "    mrna.append(indices_of_mrna[0])\n",
    "    mrna.append(indices_of_mrna[0] + indices_of_mrna.shape[0])\n",
    "    message(\"The columns for the mRNA level are:\" + str(mrna))\n",
    "    mirna.append(indices_of_mirna[0])\n",
    "    mirna.append(indices_of_mirna[0] + indices_of_mirna.shape[0])\n",
    "    message(\"The columns for the miRNA level are:\" + str(mirna))\n",
    "    methylation.append(indices_of_methylation[0])\n",
    "    methylation.append(indices_of_methylation[0] + indices_of_methylation.shape[0])\n",
    "    message(\"The columns for the DNA methylation level are:\"+str(methylation))\n",
    "\n",
    "    all_levels = []\n",
    "    all_levels.append(mrna)\n",
    "    all_levels.append(mirna)\n",
    "    all_levels.append(methylation)\n",
    "    return all_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "821ddec2-8bef-4ca0-9c89-9fa0b68a0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def omic_level_indices():\n",
    "#     \"\"\"\n",
    "#     Returns the columns corresponding to each omic level\n",
    "#     \"\"\"\n",
    "#     feature_names = getFeatureNames()\n",
    "\n",
    "#     #search for elements that start with \"ENSG\" and also include \".\"\n",
    "#     indices_of_mrna = [index for index, element in enumerate(test) if element.startswith(\"ENSG\") and \".\" in element]\n",
    "#     #search for elements that start with \"hsa\"\n",
    "#     indices_of_mirna = [index for index, element in enumerate(test) if element.startswith(\"hsa\") ]\n",
    "#     #search for elements that start with \"ENSG\" and don't include \".\"\n",
    "#     indices_of_methylation = [index for index, element in enumerate(test) if element.startswith(\"ENSG\") and \".\" not in element]\n",
    "\n",
    "#     mrna = []\n",
    "#     mirna = []\n",
    "#     methylation = []\n",
    "\n",
    "#     mrna.append(indices_of_mrna[0])\n",
    "#     mrna.append(indices_of_mrna[0] + len(indices_of_mrna))\n",
    "#     message(\"The columns for the mRNA level are:\"+str(mrna))\n",
    "#     mirna.append(indices_of_mirna[0])\n",
    "#     mirna.append(indices_of_mirna[0] + len(indices_of_mirna))\n",
    "#     message(\"The columns for the miRNA level are:\"+str(mirna))\n",
    "#     methylation.append(indices_of_methylation[0])\n",
    "#     methylation.append(indices_of_methylation[0] +len(indices_of_methylation))\n",
    "#     message(\"The columns for the DNA methylation level are:\"+str(methylation))\n",
    "\n",
    "#     all_levels = []\n",
    "#     all_levels.append(mrna)\n",
    "#     all_levels.append(mirna)\n",
    "#     all_levels.append(methylation)\n",
    "#     return all_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6dc7de-b262-49ce-9be5-d2aa666941b9",
   "metadata": {},
   "source": [
    "## remove_empty_cols_from_control is only to run postProcessFeatures\n",
    "## probably will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77e38181-0357-4bf8-b657-59257b847589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_empty_cols_from_control(mFeatures, control_m):\n",
    "#     ## it removes the columns that have only nan in control from all data and control data\n",
    "#     # Check which elements are NaN for all the data\n",
    "#     nan_mask_all = np.isnan(mFeatures)\n",
    "    \n",
    "#     # Check if all elements in each column are NaN\n",
    "#     columns_with_nan_all = np.all(nan_mask_all, axis=0)\n",
    "    \n",
    "#     # Get the indices of columns with only NaN values\n",
    "#     column_indices_all = np.where(columns_with_nan_all)[0]\n",
    "    \n",
    "#     print(\"Columns with only NaN values in all data:\", len(column_indices_all))\n",
    "    \n",
    "#     # Check which elements are NaN for control data\n",
    "#     nan_mask_control = np.isnan(control_m)\n",
    "    \n",
    "#     # Check if all elements in each column are NaN\n",
    "#     columns_with_nan_control = np.all(nan_mask_control, axis=0)\n",
    "    \n",
    "#     # Get the indices of columns with only NaN values\n",
    "#     column_indices_control = np.where(columns_with_nan_control)[0]\n",
    "    \n",
    "#     print(\"Columns with only NaN values in control matrix:\", len(column_indices_control))\n",
    "    \n",
    "#     # find common columns with only nan between all data and cintrol \n",
    "#     common_cols = np.intersect1d(column_indices_all, column_indices_control)\n",
    "#     print(\"The common elements are:\", len(common_cols))\n",
    "    \n",
    "#     # find columns that have only nan in control but haven't in all data\n",
    "#     diff_cols = np.setdiff1d(column_indices_control, column_indices_all)\n",
    "#     print(\"The different elements are:\", diff_cols)\n",
    "    \n",
    "#     diff_cols_matrix = mFeatures[:, diff_cols]\n",
    "#     # print(\"The matrix with the columns from diff_cols:\")\n",
    "#     # print(diff_cols_matrix)\n",
    "    \n",
    "#     nan_diff_elements = count_nan_per_column(diff_cols_matrix)\n",
    "#     print(\"The percentage of NaN in the different element: \", (nan_diff_elements/np.shape(diff_cols_matrix)[0])*100)\n",
    "\n",
    "\n",
    "#     # Use boolean indexing to get the submatrix with columns removed\n",
    "#     mFeatures = mFeatures[:, ~np.isin(np.arange(mFeatures.shape[1]), column_indices_control)]\n",
    "#     control_m = control_m[:, ~np.isin(np.arange(control_m.shape[1]), column_indices_control)]\n",
    "#     return mFeatures, control_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04cfae14-716e-4c96-ad87-3901fe4a39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataByControl(mFeaturesToNormalize, mControlData, logScale=True):\n",
    "    \"\"\"\n",
    "    Calculates relative change per feature, transforming also to a log 2 norm/scale\n",
    "\n",
    "    :param mFeaturesToNormalize: The matrix of features to normalize.\n",
    "    :param mControlData: The control data sub-matrix.\n",
    "    :param logScale: If True, log scaling will occur to the result. Default: True.\n",
    "    :return: The normalized and - possibly - log scaled version of the input feature matrix.\n",
    "    \"\"\"\n",
    "    message(\"Normalizing based on control set...\")\n",
    "\n",
    "    # Make a copy of the input matrix so that you do not make changes to the input matrix\n",
    "    mFeaturesToNormalize = np.copy(mFeaturesToNormalize)\n",
    "    \n",
    "    ## np.nanmean for all the columns except the last one with tumor stage\n",
    "    centroid = np.nanmean(mControlData[:, :], 0)\n",
    "    ##centroid variable will contain an array of mean values for each column in mControlData, with NaN values ignored in the calculation\n",
    "    \n",
    "    ## keep for calculation all the columns except the last one with tumor stage\n",
    "    mFeaturesForCalculation = mFeaturesToNormalize[:, :]\n",
    "    \n",
    "    # Using percentile change instead of ratio, to avoid lower bound problems\n",
    "    NormalizedmFeatures = ((mFeaturesForCalculation - centroid) + 10e-8) / (centroid + 10e-8)\n",
    "    ## Relative change apo diplwmatikh\n",
    "    ##+ 10e-8 Adds a small constant value to avoid division by zero\n",
    "\n",
    "    # DEBUG LINES\n",
    "    message(\"Data shape before normalization: %s\" % (str(np.shape(mFeaturesToNormalize))))\n",
    "\n",
    "    ## insert the results to the first columns of the matrix and keep the last column with tumor stage  \n",
    "    # mFeaturesToNormalize[:, :] = NormalizedmFeatures\n",
    "    mOut = NormalizedmFeatures\n",
    "    #############\n",
    "    if logScale:\n",
    "        mOut = np.log2(2.0 + mOut)  # Ascertain positive numbers\n",
    "        ## base-2 logarithm transformation after adding 2.0 to each element\n",
    "    # DEBUG LINES\n",
    "    message(\"Data shape after normalization: %s\" % (str(np.shape(mOut))))\n",
    "    #############\n",
    "    message(\"Normalizing based on control set... Done.\")\n",
    "    return mOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd02460c-be5a-445d-80a9-4fcf0eb50962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAOnControl():\n",
    "    \"\"\"\n",
    "    Apply and visualize PCA on control data.\n",
    "    \"\"\"\n",
    "\n",
    "    message(\"Opening file...\")\n",
    "    mFeatures_noNaNs, vClass, sampleIDs, feat_names = initializeFeatureMatrices(False, True)##return: The initial feature matrix of the cases/instances.\n",
    "    mFeatures_noNaNs = getControlFeatureMatrix(mFeatures_noNaNs, vClass)##epistrefei subset tou sunolikou matrix me mono osa exoun Solid_Tissue_Normal\n",
    "    message(\"Opening file... Done.\")\n",
    "    X, pca3DRes = getPCA(mFeatures_noNaNs, 3)##epistrefei transformed matrix X represents the data in a lower-dimensional space kai \n",
    "                                            ##pca3DRes einai pca object me to apotelesma ths pca me information about the PCA transformation, including the explained variance ratio\n",
    "    fig = draw3DPCA(X, pca3DRes)## epistrefei to plot\n",
    "    fig.savefig(\"controlPCA3D.pdf\", bbox_inches='tight')##save to plot se PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7360a02b-2f97-47e0-bf5f-b83e6667f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAOnTumor():\n",
    "    \"\"\"\n",
    "    Apply and visualize PCA on tumor data.\n",
    "    \"\"\"\n",
    "    message(\"Opening file...\")\n",
    "    mFeatures_noNaNs, vClass, sampleIDs, feat_names = initializeFeatureMatrices(False, True)##return: The initial feature matrix of the cases/instances.\n",
    "    mFeatures_noNaNs = getNonControlFeatureMatrix(mFeatures_noNaNs, vClass)##epistrefei subset tou sunolikou matrix me mono osa einai non control\n",
    "    message(\"Opening file... Done.\")\n",
    "    X, pca3DRes = getPCA(mFeatures_noNaNs, 3)##epistrefei transformed matrix X represents the data in a lower-dimensional space kai \n",
    "                                            ##pca3DRes einai pca object me to apotelesma ths pca me information about the PCA transformation, including the explained variance ratio\n",
    "\n",
    "    fig = draw3DPCA(X, pca3DRes)## epistrefei to plot\n",
    "\n",
    "    fig.savefig(\"tumorPCA3D.pdf\", bbox_inches='tight')##save to plot se PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e85b88d8-b688-4d85-b06f-8c66d5f6622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPCA(mFeatures_noNaNs, n_components=3):\n",
    "    \"\"\"\n",
    "    Return the PCA outcome given an array of instances.\n",
    "\n",
    "    :param mFeatures_noNaNs: The array to analyze.\n",
    "    :param n_components: The target number of components.\n",
    "    :return: The PCA transformation result as a matrix.\n",
    "    \"\"\"\n",
    "    pca = decomposition.PCA(n_components)##specify number of components\n",
    "    pca.fit(mFeatures_noNaNs)##calculates the principal components and their variance explained\n",
    "    X = pca.transform(mFeatures_noNaNs)##transformed matrix X represents the data in a lower-dimensional space\n",
    "\n",
    "    return X, pca##epistrefei transformed matrix X represents the data in a lower-dimensional space kai pca object pou exei specified number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53690934-97e4-4a3d-8775-cd5c27b3071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw3DPCA(X, pca3DRes, c=None, cmap=plt.cm.gnuplot, spread=False):\n",
    "    ##c: This argument allows for different classes to be color-coded in the scatter plot. It is an optional argument.\n",
    "    ##cmap: The colormap to be used for coloring the data points\n",
    "    ##spread: A boolean flag that, if set to True, applies the QuantileTransformer to spread out the data distribution. This can be useful for better visualization of the spread of data points in the plot.\n",
    "    \"\"\"\n",
    "    Draw a 3D PCA given, allowing different classes coloring.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Percentage of variance explained for each components\n",
    "    message('explained variance ratio (first 3 components): %s'\n",
    "            % str(pca3DRes.explained_variance_ratio_))##pca3DRes einai pca object me to apotelesma ths pca me information about the PCA transformation, including the explained variance ratio\n",
    "   \n",
    "    if spread:\n",
    "        X = QuantileTransformer(output_distribution='uniform').fit_transform(X)##QuantileTransformer is a data transformation technique used for scaling and normalizing data. It transforms the data in such a way that the transformed values have a specific probability distribution, often a uniform distribution\n",
    "       \n",
    "    fig = plt.figure(figsize =(15, 15))\n",
    "    plt.clf()##clears the current figure. It's often used to start with a fresh, empty canvas for the new plot\n",
    "    ax = fig.add_subplot(111, projection='3d')##The 111 argument is shorthand for a single subplot in a 1x1 grid. The projection='3d' argument specifies that this is a 3D plot\n",
    "    ax.scatter(X[:, 0], X[:, 1], X[:, 2], edgecolor='k', c=c, cmap=cmap, depthshade=False, s=100)## data points specified by the three arrays X[:, 0], X[:, 1], and X[:, 2] along the X, Y, and Z axes\n",
    "    ax.set_xlabel(\"X coordinate (%4.2f)\" % (pca3DRes.explained_variance_ratio_[0]), fontsize=20) ## set fontsize\n",
    "    ax.set_ylabel(\"Y coordinate (%4.2f)\" % (pca3DRes.explained_variance_ratio_[1]), fontsize=20)\n",
    "    ax.set_zlabel(\"Z coordinate (%4.2f)\" % (pca3DRes.explained_variance_ratio_[2]), fontsize=20)\n",
    "    ax.set_xticklabels([])##remove tick labels\n",
    "    ax.set_yticklabels([])##Tick marks are the small lines or points on an axis that help users read and interpret the data on the plot.\n",
    "    ax.set_zticklabels([])\n",
    "\n",
    "    fig.show()\n",
    "    return fig## epistrefei to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bb5977a-2622-4228-974a-b8f322a9cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAOnAllData():\n",
    "    \"\"\"\n",
    "    Applies and visualizes PCA on all data.\n",
    "    \"\"\"\n",
    "    #!!!!!!!!!!!! einai gia command line\n",
    "    bResetFiles = False\n",
    "    # if len(sys.argv) > 1: ## checks if there are more than one command-line arguments\n",
    "    #     if \"-resetFiles\" in sys.argv: ## checks if the string \"-resetFiles\" is present in the list of command-line arguments\n",
    "    #         bResetFiles = True\n",
    "\n",
    "    # Initialize feature matrices\n",
    "    mFeatures_noNaNs, vClass, sampleIDs, feat_names = initializeFeatureMatrices(bResetFiles=bResetFiles, bPostProcessing=True)\n",
    "    ##return: The initial feature matrix of the cases/instances.\n",
    "    message(\"Applying PCA...\")\n",
    "    X, pca3D = getPCA(mFeatures_noNaNs, 3)\n",
    "\n",
    "    # Spread\n",
    "    message(\"Applying PCA... Done.\")\n",
    "\n",
    "    # Percentage of variance explained for each components\n",
    "    message('explained variance ratio (first 3 components): %s'\n",
    "            % str(pca3D.explained_variance_ratio_))\n",
    "\n",
    "    message('3 components values: %s'\n",
    "            % str(X))\n",
    "\n",
    "    message(\"Plotting PCA graph...\")\n",
    "    # Assign colors\n",
    "    aCategories, y = np.unique(vClass, return_inverse=True)\n",
    "    ##aCategories contains the unique categories or labels from the original vClass array.\n",
    "    ##y is an array of integers that represent the indices of the unique categories in aCategories. Each element in y corresponds to a category in vClass.\n",
    "\n",
    "    draw3DPCA(X, pca3D, c=y / 2)\n",
    "    # DEBUG LINES\n",
    "    message(\"Returning categories: \\n %s\" % (str(aCategories)))\n",
    "    message(\"Returning categorical vector: \\n %s\" % (str(y)))\n",
    "    message(\"Plotting PCA graph... Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ea1fc3e-4b42-4ccf-ba4c-6432ab6396ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClusterAllData():\n",
    "    \"\"\"\n",
    "    Creates k-means-based clustering of the control and tumor data, visualizing the results in a PCA-based 3D space.\n",
    "    \"\"\"\n",
    "    # Initialize feature matrices\n",
    "    mFeatures_noNaNs, vClass, sampleIDs, feat_names = initializeFeatureMatrices(bResetFiles=False, bPostProcessing=True)\n",
    "    ##return: The initial feature matrix of the cases/instances.\n",
    "    message(\"Separating instances per class...\")\n",
    "    # Perform clustering, initializing the clusters with a control and a patient\n",
    "    # Set starting points\n",
    "    npaControlFeatures = getControlFeatureMatrix(mFeatures_noNaNs, vClass)##return: The subset of the data matrix, reflecting only control cases/instances.\n",
    "    npaNonControlFeatures = getNonControlFeatureMatrix(mFeatures_noNaNs, vClass)##return: The subset of the feature matrix, corresponding to non-control (i.e. tumor) data\n",
    "\n",
    "    npInitialCentroids = np.array([np.nanmedian(npaControlFeatures[:, :], 0),\n",
    "                                   np.nanmedian(npaNonControlFeatures[:, :], 0)])\n",
    "    ##np.nanmedian(array, axis=0) computes the median along each column (axis 0) of the given array, while ignoring NaN (Not-a-Number) values.\n",
    "    message(\"Separating instances per class... Done.\")\n",
    "\n",
    "    message(\"Applying k-means...\")\n",
    "    # Perform clustering\n",
    "    clusterer = KMeans(2, init=npInitialCentroids, n_init=1)\n",
    "    ## K-means clustering, npInitialCentroids are the initial centroids that you previously calculated using the median values.\n",
    "    ##2 specifies the number of clusters you want to create, n_init=1 sets the number of times the k-means algorithm will be run with different centroid seeds. In this case, it's only run once.\n",
    "    y_pred = clusterer.fit_predict(mFeatures_noNaNs)\n",
    "    ##fits the KMeans model to your data and predicts the cluster labels\n",
    "    message(\"Applying k-means... Done.\")\n",
    "\n",
    "    message(\"Applying PCA for visualization...\")\n",
    "    X, pca3D = getPCA(mFeatures_noNaNs, 3)##epistrefei transformed matrix X represents the data in a lower-dimensional space kai pca object pou exei specified number of components\n",
    "    # X = QuantileTransformer(output_distribution='uniform').fit_transform(X)\n",
    "\n",
    "    message(\"Applying PCA for visualization... Done.\")\n",
    "\n",
    "    draw3DPCA(X, pca3D, c=y_pred)\n",
    "    aCategories, y = np.unique(vClass, return_inverse=True)## np.unique(vClass, return_inverse=True) returns a tuple. The first element of the tuple (aCategories) contains the unique elements of vClass, and the second element (y) contains an integer array such that each element of vClass is replaced by its index in the unique set of elements.\n",
    "    draw3DPCA(X, pca3D, c=y)\n",
    "    ##Draw a 3D PCA given, allowing different classes coloring.\n",
    "    message(\"Plotting... Done.\")\n",
    "\n",
    "    # Calculate performance (number/precent of misplaced controls, number/precent of misplaced tumor samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e74dec7f-4a30-4043-a710-28c21b5f3008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, y):\n",
    "    \"\"\"\n",
    "    Calculates and outputs the performance of classification, through 10-fold cross-valuation, given a set of feature vectors and a set of labels.\n",
    "    :param X: The feature vector matrix.\n",
    "    :param y: The labels.\n",
    "    \"\"\"\n",
    "    classifier = DecisionTreeClassifier() ## Creates an instance of a Decision Tree classifier.\n",
    "    scores = cross_val_score(classifier, X, y, cv=min(10, len(y))); ##Performs 10-fold cross-validation using the Decision Tree classifier. \n",
    "    ## The function calculates the average performance score over the 10 folds and also outputs the standard deviation of the scores\n",
    "    ## (min(10, len(y))) is used as the number of folds to ensure a minimum of 10 folds if the dataset is smaller.\n",
    "    message(\"Avg. Performanace: %4.2f (st. dev. %4.2f) \\n %s\" % (np.mean(scores), np.std(scores), str(scores)))\n",
    "\n",
    "    # Output model\n",
    "    classifier.fit(X, y)\n",
    "    dot_data = tree.export_graphviz(classifier, out_file=None) ## Creates DOT-format data representing the structure of the trained Decision Tree.\n",
    "    graph = graphviz.Source(dot_data) ## Creates a graph from the DOT-format data.\n",
    "    graph.render(\"Rules\") ## takes the DOT-format data of the Decision Tree, uses the 'dot' program internally (from Graphviz), \n",
    "    ## and generates an image file named \"Rules\" with an appropriate file extension (e.g., \"Rules.png\" or \"Rules.pdf\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f86f834f-8c80-4d0d-b6d6-4a010f120dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureGraph(mAllData, saFeatures, dEdgeThreshold=0.30, bResetGraph=True, dMinDivergenceToKeep=np.log2(10e5)):\n",
    "    \"\"\"\n",
    "    Returns the overall feature graph, indicating interconnections between features.\n",
    "\n",
    "    :param mAllData: The matrix containing all case/instance data.\n",
    "    :param dEdgeThreshold: The threshold of minimum correlation required to keep an edge.\n",
    "    :param bResetGraph: If True, recompute correlations, else load from disc (if available). Default: True.\n",
    "    :param dMinDivergenceToKeep: The threshold of deviation, indicating which features it makes sense to keep.\n",
    "    Features with a deviation below this value are considered trivial. Default: log2(10e5).\n",
    "    :return: The graph containing only useful features and their connections, indicating correlation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if bResetGraph:\n",
    "            raise Exception(\"User requested graph recreation.\")\n",
    "\n",
    "        message(\"Trying to load graph...\")\n",
    "        g = nx.Graph()\n",
    "        g = read_multiline_adjlist(Prefix + \"graphAdjacencyList.txt\", create_using=g) ## reads the graph from a file using read_multiline_adjlist\n",
    "        with open(Prefix + \"usefulFeatureNames.pickle\", \"rb\") as fIn: ## reads a list of useful feature names from a pickle file\n",
    "            saUsefulFeatureNames = pickle.load(fIn)\n",
    "        message(\"Trying to load graph... Done.\")\n",
    "        return g, saUsefulFeatureNames\n",
    "    except Exception as e:\n",
    "        message(\"Trying to load graph... Failed:\\n%s\\n Recomputing...\" % (str(e)))\n",
    "\n",
    "    # DEBUG LINES\n",
    "    message(\"Got data of size %s.\" % (str(np.shape(mAllData))))\n",
    "    message(\"Extracting graph...\")\n",
    "    #############\n",
    "    # Init graph\n",
    "\n",
    "    # Determine meaningful features (with a divergence of more than MIN_DIVERGENCE from the control mean)\n",
    "\n",
    "    iFeatureCount = np.shape(mAllData)[1] ## the number of features in the input data mAllData\n",
    "    mMeans = np.nanmean(mAllData, 0)  # Ignore nans ##computes the mean of each feature, ignoring NaN values.\n",
    "\n",
    "    # Q1 Chris: is this the step where we apply the threshold? What is the threshold?\n",
    "    # So, basically keep in vUseful, only the features that their value is greater than dMinDivergenceToKeep\n",
    "    vUseful = [abs(mMeans[iFieldNum]) - dMinDivergenceToKeep > 0.00 for iFieldNum in range(0, iFeatureCount)] ##boolean list indicating whether each feature's absolute deviation from the mean is greater than dMinDivergenceToKeep\n",
    "\n",
    "    \n",
    "    # saFeatures = getFeatureNames()[1:iFeatureCount] ## obtaining the names of the features in the dataset\n",
    "    # REMOVED and take as input filtered features names from initializeFeatureMatrices\n",
    "   \n",
    "    saUsefulIndices = [iFieldNum for iFieldNum, _ in enumerate(saFeatures) if vUseful[iFieldNum]]\n",
    "    ## enumerate(saFeatures): The enumerate function is used to iterate over the indices and values of saFeatures. It returns pairs of index and value for each element in saFeatures.\n",
    "    ## for iFieldNum, _ in enumerate(saFeatures): This part of the list comprehension unpacks the index as iFieldNum and ignores the value (using the underscore _) since it's not needed in this context.\n",
    "    ## final result is a list (saUsefulIndices) containing the indices of features that are considered \"useful\" based on the thresholding condition. \n",
    "    saUsefulFeatureNames = [saFeatures[iFieldNum] for iFieldNum in saUsefulIndices] \n",
    "    ## extracts the names of \"useful\" features based on the indices stored in saUsefulIndices and creates a new list containing these feature names (saUsefulFeatureNames)\n",
    "    iUsefulFeatureCount = len(saUsefulIndices)\n",
    "    message(\"Keeping %d features out of %d.\" % (len(saUsefulIndices), len(saFeatures)))\n",
    "    ###############################\n",
    "\n",
    "    g = nx.Graph()\n",
    "    message(\"Adding nodes...\")\n",
    "    # Add a node for each feature\n",
    "    lIndexedNames = enumerate(saFeatures)\n",
    "    for idx in saUsefulIndices:\n",
    "        # Only act on useful features\n",
    "        g.add_node(saFeatures[idx], label=idx)\n",
    "    message(\"Adding nodes... Done.\")\n",
    "\n",
    "    # Measure correlations\n",
    "    iAllPairs = (iUsefulFeatureCount * iUsefulFeatureCount) * 0.5\n",
    "    ## (iUsefulFeatureCount * iUsefulFeatureCount) calculates the total number of possible pairs of \"useful\" features\n",
    "    ## Multiplying by 0.5 is equivalent to dividing by 2, which accounts for the fact that combinations are used (unordered pairs).\n",
    "    message(\"Routing edge calculation for %d possible pairs...\" % (iAllPairs))\n",
    "    lCombinations = itertools.combinations(saUsefulIndices, 2)\n",
    "    ## itertools.combinations generates all possible combinations of length 2 from the elements in saUsefulIndices.\n",
    "    ## Each combination represents an unordered pair of indices, which will be used to calculate correlations between pairs of \"useful\" features.\n",
    "\n",
    "    # Create queue and threads\n",
    "    threads = []\n",
    "    num_worker_threads = THREADS_TO_USE  # DONE: Use available processors\n",
    "    ## THREADS_TO_USE likely represents the desired number of worker threads to use for parallel processing.\n",
    "    qCombination = Queue(1000 * num_worker_threads)\n",
    "    ##This creates a queue (qCombination) with a maximum size of 1000 * num_worker_threads. The queue is used to pass combinations of feature indices to the worker threads for processing.\n",
    "    \n",
    "    processes = [Thread(target=addEdgeAboveThreshold, args=(i, qCombination,)) for i in range(num_worker_threads)]\n",
    "    ## This creates a list of Thread objects (processes), each corresponding to a worker thread.\n",
    "    ## The target is set to the addEdgeAboveThreshold function, which is the function that will be executed in parallel.\n",
    "    ## The args parameter is a tuple containing arguments to be passed to the addEdgeAboveThreshold function. In this case, it includes the thread index i and the queue qCombination\n",
    "    for t in processes:\n",
    "        t.daemon = True\n",
    "        #t.setDaemon(True)\n",
    "        ## This sets each thread in the processes list as a daemon thread. Daemon threads are background threads that are terminated when the main program finishes.\n",
    "        t.start()\n",
    "        ## This starts each thread in the processes list, initiating parallel execution of the addEdgeAboveThreshold function.\n",
    "\n",
    "    # Feed tasks\n",
    "    iCnt = 1\n",
    "    dStartTime = perf_counter()\n",
    "    for iFirstFeatIdx, iSecondFeatIdx in lCombinations:\n",
    "        qCombination.put((iFirstFeatIdx, iSecondFeatIdx, g, mAllData, saFeatures, iFirstFeatIdx, iSecondFeatIdx,\n",
    "                          iCnt, iAllPairs, dStartTime, dEdgeThreshold))\n",
    "        ## This line puts a tuple containing various parameters onto the queue (qCombination)\n",
    "        ##  this tuple encapsulates all the necessary information for a worker thread to calculate the correlation between two features, determine whether an edge should be added to the graph, and perform the task efficiently. \n",
    "        ## The worker threads will dequeue these tuples and execute the corresponding tasks in parallel.\n",
    "        # Wait a while if we reached full queue\n",
    "        if qCombination.full():\n",
    "            message(\"So far routed %d tasks. Waiting on worker threads to provide more tasks...\" % (iCnt))\n",
    "            time.sleep(0.05)\n",
    "\n",
    "        iCnt += 1\n",
    "    message(\"Routing edge calculation for %d possible pairs... Done.\" % (iAllPairs))\n",
    "\n",
    "    message(\"Waiting for completion...\")\n",
    "    qCombination.join()\n",
    "    ## The qCombination.join() method is used to block the program execution until all tasks in the queue (qCombination) are done. It is typically used in a scenario where multiple threads are performing parallel tasks, \n",
    "    ## and the main program needs to wait for all threads to finish their work before proceeding.\n",
    "    message(\"Total time (sec): %4.2f\" % (perf_counter() - dStartTime))\n",
    "\n",
    "    message(\"Creating edges for %d possible pairs... Done.\" % (iAllPairs))\n",
    "\n",
    "    message(\"Extracting graph... Done.\")\n",
    "\n",
    "    message(\"Removing single nodes... Nodes before removal: %d\" % (g.number_of_nodes()))\n",
    "    toRemove = [curNode for curNode in g.nodes().keys() if len(g[curNode]) == 0]\n",
    "    ## a list (toRemove) containing the nodes in the graph (g) that have no edges, meaning they are isolated nodes (nodes with degree zero). \n",
    "    ## The condition len(g[curNode]) == 0 checks if the node's degree is zero.\n",
    "    while len(toRemove) > 0:\n",
    "        g.remove_nodes_from(toRemove) ## This removes the nodes listed in toRemove from the graph g\n",
    "        toRemove = [curNode for curNode in g.nodes().keys() if len(g[curNode]) == 0] ## After removal, it updates the toRemove list with the names of nodes that are still isolated.\n",
    "        message(\"Nodes after removal step: %d\" % (g.number_of_nodes()))\n",
    "    message(\"Removing single nodes... Done. Nodes after removal: %d\" % (g.number_of_nodes()))\n",
    "\n",
    "    message(\"Saving graph...\")\n",
    "    write_multiline_adjlist(g, Prefix + \"graphAdjacencyList.txt\") ## save a file using write_multiline_adjlist\n",
    "    with open(Prefix + \"usefulFeatureNames.pickle\", \"wb\") as fOut: ## This line opens a file named \"usefulFeatureNames.pickle\" in binary write mode (\"wb\"). The with statement is used to ensure that the file is properly closed after writing.\n",
    "        pickle.dump(saUsefulFeatureNames, fOut) ## serialize the Python object saUsefulFeatureNames and write the serialized data to the file fOut. The object is serialized into a binary format suitable for storage or transmission.\n",
    "\n",
    "    message(\"Saving graph... Done.\")\n",
    "\n",
    "    message(\"Trying to load graph... Done.\")\n",
    "\n",
    "    return g, saUsefulFeatureNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe9002d1-7174-449d-a5eb-94186cd2be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEdgeAboveThreshold(i, qQueue):\n",
    "    \"\"\"\n",
    "    Helper function for parallel execution. It adds an edge between two features in the overall feature correlation\n",
    "    graph, if the correlation exceeds a given level. All parameters are provided via a task Queue.\n",
    "    :param i: The number of the executing thread.\n",
    "    :param qQueue: The Queue object containing related task info.\n",
    "    \"\"\"\n",
    "    while True:## continuously retrieves tasks from a qQueue object until it encounters a None task\n",
    "        # Get next feature index pair to handle\n",
    "        params = qQueue.get()\n",
    "        # If empty, stop\n",
    "        if params is None:\n",
    "            message(\"Reached and of queue... Stopping.\")\n",
    "            break\n",
    "        ## unpacks the parameters received from the queue, including feature indices (iFirstFeatIdx and iSecondFeatIdx), a graph (g), the entire feature data matrix (mAllData), an array of feature names (saFeatures), counters, a start time, and a correlation threshold\n",
    "        iFirstFeatIdx, iSecondFeatIdx, g, mAllData, saFeatures, iFirstFeatIdx, iSecondFeatIdx, iCnt, iAllPairs, dStartTime, dEdgeThreshold = params\n",
    "\n",
    "        # DEBUG LINES\n",
    "        ## provide information about the progress of the correlation calculations. This information includes a progress dot for every 1000 iterations and more detailed information for every 10000 iterations\n",
    "        if iCnt != 0 and (iCnt % 1000 == 0):\n",
    "            progress(\".\")\n",
    "            if iCnt % 10000 == 0 and iCnt != 0:\n",
    "                dNow = perf_counter()\n",
    "                dRate = ((dNow - dStartTime) / iCnt)\n",
    "                dRemaining = (iAllPairs - iCnt) * dRate\n",
    "                message(\"%d (Estimated remaining (sec): %4.2f - Working at a rate of %4.2f pairs/sec)\\n\" % (\n",
    "                    iCnt, dRemaining, 1.0 / dRate))\n",
    "\n",
    "        iCnt += 1\n",
    "        #############\n",
    "        ## calculates the Pearson correlation coefficient (fCurCorr) between the selected feature columns (vFirstRepr and vSecondRepr) using the pearsonr function\n",
    "        # Fetch feature columns and calculate pearson\n",
    "        vFirstRepr = mAllData[:, iFirstFeatIdx]\n",
    "        vSecondRepr = mAllData[:, iSecondFeatIdx]\n",
    "        fCurCorr = pearsonr(vFirstRepr, vSecondRepr)[0]\n",
    "        ## If the correlation is above the specified threshold (dEdgeThreshold), it adds an edge to the graph (g) connecting the corresponding features with a weight representing the rounded correlation coefficient\n",
    "        # Add edge, if above threshold\n",
    "        if fCurCorr > dEdgeThreshold:\n",
    "            g.add_edge(saFeatures[iFirstFeatIdx], saFeatures[iSecondFeatIdx], weight=round(fCurCorr * 100) / 100)## dtrogg se 2 dekadika psifia\n",
    "\n",
    "        # Update queue\n",
    "        ## The function then updates the counters and signals that the task is done\n",
    "        qQueue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6aa938e8-7f78-4dbb-956e-74e6873cadf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawAndSaveGraph(gToDraw, bShow=True, sPDFFileName=\"corrGraph.pdf\", bSave = True):\n",
    "    \"\"\"\n",
    "    Draws and displays a given graph using graphviz.\n",
    "\n",
    "    :param gToDraw: The graph to draw.\n",
    "    \"\"\"\n",
    "    # Calculate the size based on the number of edges (you can adjust this as needed)\n",
    "    figure_size = (len(gToDraw.edges()) * 2, len(gToDraw.edges()) * 2)\n",
    "\n",
    "    plt.figure(figsize=figure_size)\n",
    "    plt.clf()\n",
    "\n",
    "    pos = graphviz_layout(gToDraw, prog='dot')  # Compute the positions of the nodes in the graph\n",
    "\n",
    "    try:\n",
    "        dNodeLabels = {}\n",
    "        # For each node\n",
    "        for nCurNode in gToDraw.nodes():\n",
    "            # Try to add weight\n",
    "            dNodeLabels[nCurNode] = \"%s (%4.2f)\" % (str(nCurNode), gToDraw.nodes[nCurNode]['weight'])\n",
    "    except KeyError:\n",
    "        # Weights could not be added, use nodes as usual\n",
    "        dNodeLabels = None\n",
    "\n",
    "    nx.draw_networkx(gToDraw, pos, arrows=False, node_size=1200, node_color=\"blue\", with_labels=True, labels=dNodeLabels, font_size=10)\n",
    "\n",
    "    # Increase the margins to ensure labels are fully visible\n",
    "    plt.margins(x=0.5, y=0.2)\n",
    "\n",
    "    # nx.draw_networkx: Draws the nodes and edges of the graph using the specified positions (pos) and other parameters\n",
    "    labels = nx.get_edge_attributes(gToDraw, 'weight')\n",
    "    \n",
    "    # Extract the 'weight' attribute from the edges of a NetworkX graph (gToDraw)\n",
    "    nx.draw_networkx_edge_labels(gToDraw, pos, edge_labels=labels)\n",
    "    # nx.draw_networkx_edge_labels: Draws labels for the edges, assuming there are 'weight' attributes associated with the edges\n",
    "    \n",
    "    \n",
    "    if bSave:\n",
    "        plt.savefig(sPDFFileName, bbox_inches='tight')## bbox_inches='tight': This parameter adjusts the bounding box around the saved figure. The argument 'tight' is used to minimize the whitespace around the actual content of the figure\n",
    "        message(\"Saving graph to file... Done.\")\n",
    "    # figure is closed after calling plt.show(), so it goes after savefig\n",
    "    if bShow:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de56eca-23d7-478f-bfc1-ef944fec1989",
   "metadata": {},
   "source": [
    "# old def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "435a16a2-57da-43d0-ae55-3e8504ca53de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drawGraph(gToDraw, bShow=True):\n",
    "#     \"\"\"\n",
    "#     Draws and displays a given graph using graphviz.\n",
    "\n",
    "#     :param gToDraw: The graph to draw.\n",
    "#     \"\"\"\n",
    "#     # Calculate the size based on the number of edges (you can adjust this as needed)\n",
    "#     figure_size = (len(gToDraw.edges()) * 2, len(gToDraw.edges()) * 2)\n",
    "\n",
    "#     plt.figure(figsize=figure_size)\n",
    "#     plt.clf()\n",
    "\n",
    "#     pos = graphviz_layout(gToDraw, prog='dot')  # Compute the positions of the nodes in the graph\n",
    "\n",
    "#     try:\n",
    "#         dNodeLabels = {}\n",
    "#         # For each node\n",
    "#         for nCurNode in gToDraw.nodes():\n",
    "#             # Try to add weight\n",
    "#             dNodeLabels[nCurNode] = \"%s (%4.2f)\" % (str(nCurNode), gToDraw.nodes[nCurNode]['weight'])\n",
    "#     except KeyError:\n",
    "#         # Weights could not be added, use nodes as usual\n",
    "#         dNodeLabels = None\n",
    "\n",
    "#     nx.draw_networkx(gToDraw, pos, arrows=False, node_size=1200, node_color=\"blue\", with_labels=True, labels=dNodeLabels, font_size=10)\n",
    "\n",
    "#     # Increase the margins to ensure labels are fully visible\n",
    "#     plt.margins(x=0.5, y=0.2)\n",
    "\n",
    "#     # nx.draw_networkx: Draws the nodes and edges of the graph using the specified positions (pos) and other parameters\n",
    "#     labels = nx.get_edge_attributes(gToDraw, 'weight')\n",
    "    \n",
    "#     # Extract the 'weight' attribute from the edges of a NetworkX graph (gToDraw)\n",
    "#     nx.draw_networkx_edge_labels(gToDraw, pos, edge_labels=labels)\n",
    "    \n",
    "#     # nx.draw_networkx_edge_labels: Draws labels for the edges, assuming there are 'weight' attributes associated with the edges\n",
    "#     if bShow:\n",
    "#         plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe446c45-fb23-4d72-a44a-30a86da0c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def showAndSaveGraph(gToDraw, sPDFFileName=\"corrGraph.pdf\",bShow = True, bSave = True ):\n",
    "#     \"\"\"\n",
    "#     Draws and displays a given graph, also saving it to a given file.\n",
    "#     :param gToDraw: The graph to draw and save.\n",
    "#     :param sPDFFileName:  The output filename. Default: corrGraph.pdf.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if bShow:\n",
    "#         message(\"Displaying graph...\")\n",
    "#         drawGraph(gToDraw, bShow)\n",
    "#         message(\"Displaying graph... Done.\")\n",
    "#     else:\n",
    "#         message(\"Ignoring graph display as requested...\")\n",
    "\n",
    "#     if bSave:\n",
    "#         message(\"Saving graph to file...\")\n",
    "#         try:\n",
    "#             if bSave:\n",
    "#                 plt.savefig(sPDFFileName, bbox_inches='tight')## bbox_inches='tight': This parameter adjusts the bounding box around the saved figure. The argument 'tight' is used to minimize the whitespace around the actual content of the figure\n",
    "#                 message(\"Saving graph to file... Done.\")\n",
    "#         except Exception as e:\n",
    "#             print(\"Could not save file! Exception:\\n%s\\n\"%(str(e)))\n",
    "#             print(\"Continuing normally...\")\n",
    "#     else:\n",
    "#         message(\"Ignoring graph saving as requested...\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7df89e6d-942a-4f55-92c4-19c3754514dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGraphAndData(bResetGraph=False, dMinDivergenceToKeep=np.log2(10e6), dEdgeThreshold=0.3,\n",
    "                    bResetFiles=False, bPostProcessing=True, bNormalize=True, bNormalizeLog2Scale=True): # TODO: dMinDivergenceToKeep: Add as parameter\n",
    "    \"\"\"\n",
    "    Loads the feature correlation graph and all feature data.\n",
    "    :param bResetGraph: If True, recalculate graph, else load from disc. Default: False.\n",
    "    :param dMinDivergenceToKeep: The threshold of data deviation to consider a feature useful. Default: log2(10e6).\n",
    "    :param dEdgeThreshold: The minimum correlation between features to consider the connection useful. Default: 0.3.\n",
    "    :param bResetFiles: If True, clear initial feature matrix serialization and re-parse CSV file. Default: False.\n",
    "    :param bPostProcessing: If True, apply preprocessing to remove NaNs, etc. Default: True.\n",
    "    :param bNormalize: If True, apply normalization to remove NaNs, etc. Default: True.\n",
    "    :param bNormalizeLog2Scale: If true, after normalization apply log2 scale to feature values.\n",
    "    :return: A tuple of the form (feature correlation graph, all feature matrix, instance/case class matrix,\n",
    "        important feature names list)\n",
    "    CV update:\n",
    "    :return: A tuple of the form (feature correlation graph, all feature matrix, instance/case class matrix,\n",
    "        important feature names list, sample ids)\n",
    "    \"\"\"\n",
    "    # Do mFeatures_noNaNs has all features? Have we applied a threshold to get here?\n",
    "    mFeatures_noNaNs, vClass, sampleIDs, feat_names = initializeFeatureMatrices(bResetFiles=bResetFiles, bPostProcessing=bPostProcessing,\n",
    "                                                         bNormalize=bNormalize, bNormalizeLog2Scale=bNormalizeLog2Scale)\n",
    "    gToDraw, saRemainingFeatureNames = getFeatureGraph(mFeatures_noNaNs, feat_names, dEdgeThreshold=dEdgeThreshold, bResetGraph=bResetGraph, dMinDivergenceToKeep=dMinDivergenceToKeep)\n",
    "\n",
    "    return gToDraw, mFeatures_noNaNs, vClass, saRemainingFeatureNames, sampleIDs, feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a186c69-d3c5-4766-8878-934a072f04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanDegreeCentrality(gGraph):\n",
    "    \"\"\"\n",
    "    Returns the average of the degree centralities of the nodes of a given graph.\n",
    "    :param gGraph: The given graph.\n",
    "    :return: The mean of degree centralities.\n",
    "    \"\"\"\n",
    "    mCentralities = list(nx.degree_centrality(gGraph).values())\n",
    "    return np.mean(mCentralities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "355ee544-24c6-439c-8e49-93030bbc8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGraphVector(gGraph):\n",
    "    \"\"\"\n",
    "    Represents a given graph as a vector/matrix, where each feature represents a graph description metric.\n",
    "    :param gGraph: The graph to represent.\n",
    "    :return: The feature vector, consisting of: #edges,#nodes, mean node degree centrality, number of cliques,\n",
    "    average node connectivity, mean pair-wise shortest paths of connected nodes.\n",
    "    \"\"\"\n",
    "    # DEBUG LINES\n",
    "    message(\"Extracting graph feature vector...\")\n",
    "    ## resulting feature vector includes various graph metrics (Number of Edges, Number of Nodes, \n",
    "    ## Mean Node Degree Centrality, Number of Cliques, Average Node Connectivity, Mean Pair-wise Shortest Paths of Connected Nodes\n",
    "    ## A clique in a graph is a subset of nodes where every two distinct nodes are adjacent (connected by an edge). In other words, a clique is a fully connected subgraph.\n",
    "    ## counts how many cliques exist in the given graph.\n",
    "    mRes = np.asarray(\n",
    "        [len(gGraph.edges()), len(gGraph.nodes()),\n",
    "         np.mean(np.array(list(nx.algorithms.centrality.degree_alg.degree_centrality(gGraph).values()))),\n",
    "         # Avg deg centrality\n",
    "         \n",
    "         #nx.algorithms.clique.graph_number_of_cliques(gGraph),\n",
    "         nx.algorithms.clique.number_of_cliques(gGraph),\n",
    "         \n",
    "         # TODO: Shortest path does NOT work.; revisit if needed\n",
    "         # nx.algorithms.shortest_paths.unweighted.all_pairs_shortest_path_length(gGraph),\n",
    "         # getAvgShortestPath(gGraph)\n",
    "         nx.algorithms.connectivity.connectivity.average_node_connectivity(gGraph),\n",
    "         nx.average_shortest_path_length(gGraph)\n",
    "         \n",
    "         # old AvgShortestPath\n",
    "         # np.mean([np.mean(list(x[1].values())) for x in\n",
    "         #          list(nx.algorithms.shortest_paths.unweighted.all_pairs_shortest_path_length(gGraph))]),\n",
    "         ]\n",
    "    )\n",
    "    # DEBUG LINES\n",
    "    message(\"Extracting graph feature vector... Done.\")\n",
    "\n",
    "    return mRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8cf9f59-b077-49e8-a8a4-95034f745b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSampleGraphVectors(gMainGraph, mFeatures_noNaNs, saRemainingFeatureNames, sampleIDs, bResetFeatures=True,\n",
    "                          numOfSelectedSamples=-1, bShowGraphs=True, bSaveGraphs=True):\n",
    "    \"\"\"\n",
    "    Extracts the graph feature vectors of a given set of instances/cases.\n",
    "    :param gMainGraph: The overall feature correlation graph.\n",
    "    :param mFeatures_noNaNs: The (clean from NaNs) feature matrix of instances/cases.\n",
    "    :param saRemainingFeatureNames: The list of useful feature names.\n",
    "    :param bResetFeatures: If True, features will be re-calculated. Otherwise, they will be loaded from an intermediate\n",
    "    file. Default: True.\n",
    "    :param numOfSelectedSamples: Allows working on a subset of the data. If -1, then use all data. Else use the given\n",
    "    number of instances (half of which are taken from the first instances in mFeatures_noNaNs, while half from the\n",
    "    last ones). Default: -1 (i.e. all samples).\n",
    "    :return: A matrix containing the graph feature vectors of the selected samples.\n",
    "    \"\"\"\n",
    "    ##responsible for extracting graph feature vectors of a given set of instances or cases\n",
    "    # Get all sample graph vectors\n",
    "    try:\n",
    "        message(\"Trying to load graph feature matrix...\")\n",
    "        if bResetFeatures:\n",
    "            raise Exception(\"User requested rebuild of features.\")\n",
    "        with open(Prefix + \"graphFeatures.pickle\", \"rb\") as fIn:## Opens the file in binary read mode\n",
    "            mGraphFeatures = pickle.load(fIn)\n",
    "        message(\"Trying to load graph feature matrix... Done.\")\n",
    "    except Exception as e:\n",
    "        message(\"Trying to load graph feature matrix... Failed:\\n%s\" % (str(e)))\n",
    "        message(\"Computing graph feature matrix...\")\n",
    "\n",
    "        if (numOfSelectedSamples < 0): ## If numOfSelectedSamples is less than 0, it means that all samples should be used\n",
    "            mSamplesSelected = mFeatures_noNaNs\n",
    "        else:\n",
    "            mSamplesSelected = np.concatenate((mFeatures_noNaNs[0:int(numOfSelectedSamples / 2)][:], ## Selects the first half of the samples, from index 0 to half of numOfSelectedSamples\n",
    "                                               mFeatures_noNaNs[-int(numOfSelectedSamples / 2):][:]), axis=0) ##Selects the second half of the samples, from the negative index (counting from the end) equivalent to half of numOfSelectedSamples to the end.\n",
    "\n",
    "        message(\"Extracted selected samples:\\n\" + str(mSamplesSelected[:][0:10]))\n",
    "        # Extract vectors\n",
    "        # TODO pass SampleID to generateAllSampleGraphFeatureVectors\n",
    "        mGraphFeatures = generateAllSampleGraphFeatureVectors(gMainGraph, mSamplesSelected, saRemainingFeatureNames, sampleIDs, bShowGraphs, bSaveGraphs)\n",
    "        ## This function is assumed to compute graph feature vectors for the selected samples\n",
    "        message(\"Computing graph feature matrix... Done.\")\n",
    "\n",
    "        message(\"Saving graph feature matrix...\")\n",
    "        with open(Prefix + \"graphFeatures.pickle\", \"wb\") as fOut:\n",
    "            pickle.dump(mGraphFeatures, fOut) ## Uses the pickle module to serialize (dump) the data from mGraphFeatures and write it to the opened file (fOut).\n",
    "        message(\"Saving graph feature matrix... Done.\")\n",
    "    return mGraphFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83e0131f-47c2-428f-b72e-6b5a26f1afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAllSampleGraphFeatureVectors(gMainGraph, mAllSamples, saRemainingFeatureNames, sampleIDs, bShowGraphs, bSaveGraphs):\n",
    "    \"\"\"\n",
    "    Generates graph feature vectors for all samples and returns them as a matrix.\n",
    "    :param gMainGraph: The generic graph of feature correlations.\n",
    "    :param mAllSamples: The samples to uniquely represent as graph feature vectors.\n",
    "    :param saRemainingFeatureNames: The useful features subset.\n",
    "    :return: A matrix representing the samples (rows), based on their graph representation.\n",
    "    \"\"\"\n",
    "    ########################\n",
    "    # Create queue and threads\n",
    "    threads = []\n",
    "    num_worker_threads = THREADS_TO_USE ## number of threads to be used in parallel.\n",
    "    qTasks = Queue(10 * num_worker_threads) ## A queue used to distribute tasks to worker threads.\n",
    "    for i in range(num_worker_threads):\n",
    "        t = Thread(target=getSampleGraphFeatureVector, args=(i, qTasks,bShowGraphs, bSaveGraphs,))\n",
    "        ## It creates a new thread (t) with the target set to the getSampleGraphFeatureVector function. The args parameter passes arguments to the function. \n",
    "        ## In this case, it passes the thread index i, a task queue qTasks, and two boolean values bShowGraphs and bSaveGraphs.\n",
    "        t.setDaemon(True) ##  Daemon threads are background threads that automatically exit when the main program finishes\n",
    "        t.start() ## This starts the thread, and it will execute the getSampleGraphFeatureVector function concurrently\n",
    "\n",
    "    # Count instances\n",
    "    iAllCount = np.shape(mAllSamples)[0] ## The total number of samples\n",
    "\n",
    "    # Item iterator\n",
    "    iCnt = iter(range(1, iAllCount + 1)) ## provide a sequence of numbers that can be used to track the progress or count the iterations of a loop or process.\n",
    "    dStartTime = clock()\n",
    "\n",
    "    # Init result list\n",
    "    lResList = []\n",
    "    # Add all items to queue\n",
    "    np.apply_along_axis( ##apply a function along an axis of a NumPy array (mAllSamples)\n",
    "        lambda mSample: qTasks.put((sampleIDs, lResList, gMainGraph, mSample, saRemainingFeatureNames, next(iCnt), iAllCount,\n",
    "                                    dStartTime)), 1, mAllSamples)\n",
    "    ## next(iCnt): next value from the iterator iCnt, retrieving the current sample index or iteration count.\n",
    "    ## 1, indicating that the function will be applied to each row of the array.\n",
    "    message(\"Waiting for completion...\")\n",
    "    qTasks.join() ## qTasks.join() is used to block the execution of the current thread until all tasks in the queue (qTasks) are completed\n",
    "    message(\"Total time (sec): %4.2f\" % (clock() - dStartTime))\n",
    "\n",
    "    return np.array(lResList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb4a7cc6-aeb4-49ef-a5db-1d6b278511f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSampleGraphFeatureVector(i, qQueue, bShowGraphs=True, bSaveGraphs=True):\n",
    "    \"\"\"\n",
    "    Helper parallelization function, which calculates the graph representation of a given sample.\n",
    "    :param i: The thread number calling the helper.\n",
    "    :param qQueue: A Queue, from which the execution data will be drawn. Should contain:\n",
    "    lResList -- reference to the list containing the result\n",
    "    gMainGraph -- the generic graph of feature correlations\n",
    "    mSample -- the sample to represent\n",
    "    saRemainingFeatureNames -- the list of useful feature names\n",
    "    iCnt -- the current sample count\n",
    "    iAllCount -- the number of all samples to be represented\n",
    "    dStartTime -- the time when parallelization started\n",
    "    \"\"\"\n",
    "\n",
    "    # dSample = {}\n",
    "\n",
    "    while True:\n",
    "        sampleID, lResList, gMainGraph, mSample, saRemainingFeatureNames, iCnt, iAllCount, dStartTime = qQueue.get()\n",
    "\n",
    "        # DEBUG LINES\n",
    "        message(\"Working on instance %d of %d...\" % (iCnt, iAllCount))\n",
    "        #############\n",
    "\n",
    "        # Create a copy of the graph\n",
    "        gMainGraph = copy.deepcopy(gMainGraph)\n",
    "\n",
    "        # Assign values\n",
    "        assignSampleValuesToGraphNodes(gMainGraph, mSample, saRemainingFeatureNames)\n",
    "        # Apply spreading activation\n",
    "        gMainGraph = spreadingActivation(gMainGraph, bAbsoluteMass=True)  # TODO: Add parameter, if needed\n",
    "        # Keep top performer nodes\n",
    "        gMainGraph = filterGraphNodes(gMainGraph, dKeepRatio=0.25)  # TODO: Add parameter, if needed\n",
    "        # Extract and return features\n",
    "        vGraphFeatures = getGraphVector(gMainGraph)\n",
    "        \n",
    "        message(\"Calling showAndSaveGraph for graph %s...\"%(str(sampleID)))\n",
    "        showAndSaveGraph(gMainGraph, sPDFFileName = \"SampleID%s\" %(sampleID), bShow = bShowGraphs, bSave = bSaveGraphs)\n",
    "        message(\"Calling showAndSaveGraph...Done\")\n",
    "        #  Add to common result queue\n",
    "        lResList.append(vGraphFeatures)\n",
    "\n",
    "        # Signal done\n",
    "        qQueue.task_done()##  It is typically used to indicate that a task previously retrieved from the queue has been completed.\n",
    "\n",
    "        # DEBUG LINES\n",
    "        if iCnt % 5 == 0 and (iCnt != 0):## progress update that is printed every 5 iterations of the loop\n",
    "            dNow = clock()\n",
    "            dRate = ((dNow - dStartTime) / iCnt)\n",
    "            dRemaining = (iAllCount - iCnt) * dRate\n",
    "            message(\"%d (Estimated remaining (sec): %4.2f - Working at a rate of %4.2f samples/sec)\\n\" % (\n",
    "                iCnt, dRemaining, 1.0 / dRate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e6b78-df06-47fc-b694-6be51cec224c",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e765daff-f11a-4f08-8043-0f42a3812efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignSampleValuesToGraphNodes(gGraph, mSample, saSampleFeatureNames, feat_names):\n",
    "    \"\"\"\n",
    "    Assigns values/weights to nodes of a given graph (inplace), for a given sample.\n",
    "    :param gGraph: The generic graph.\n",
    "    :param mSample: The sample which will define the feature node values/weights.\n",
    "    :param saSampleFeatureNames: The mapping between feature names and indices.\n",
    "    \"\"\"\n",
    "    # For each node\n",
    "    for nNode in gGraph.nodes():\n",
    "        # Get corresponding feature idx in sample \n",
    "        ## i changed saSampleFeatureNames with feat_names in order to get the right index of the feature name in the matrix \n",
    "        iFeatIdx = feat_names.index(nNode)\n",
    "        # Assign value of feature as node weight\n",
    "        dVal = mSample[iFeatIdx]\n",
    "        # Handle missing values as zero (i.e. non-important)\n",
    "        if dVal == np.NAN:\n",
    "            dVal = 0\n",
    "\n",
    "        gGraph.nodes[nNode]['weight'] = dVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "80f2f524-64bf-48eb-b707-359ce186c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spreadingActivation(gGraph, iIterations=100, dPreservationPercent=0.5, bAbsoluteMass=False):\n",
    "    \"\"\"\n",
    "    Applies spreading activation to a given graph.\n",
    "    :param gGraph: The graph used to apply spreading activation.\n",
    "    :param iIterations: The number of iterations for the spreading.\n",
    "    :param dPreservationPercent: The preservation of mass from each node, during the spreading.\n",
    "    :param bAbsoluteMass: If True, use absolute values of mass. Otherwise, also allow negative spreading.\n",
    "    :return: The (inplace) updated graph.\n",
    "    \"\"\"\n",
    "    message(\"Applying spreading activation...\")\n",
    "    #!!! In each iteration\n",
    "    for iIterCnt in range(iIterations):\n",
    "        #!!! For every node\n",
    "        for nCurNode in gGraph.nodes():\n",
    "            # Get max edge weight\n",
    "            dWeights = np.asarray([gGraph[nCurNode][nNeighborNode]['weight'] for nNeighborNode in gGraph[nCurNode]])\n",
    "            ## collects the weights of the edges between the nCurNode and its neighboring nodes. It does this by creating a list dWeights\n",
    "            ##p.asarray(dWeights) converts this list of edge weights into a NumPy array\n",
    "            dWeightSum = np.sum(dWeights)\n",
    "            ##sum of the edge weights stored in the dWeights \n",
    "            # For every neighbor\n",
    "            for nNeighborNode in gGraph[nCurNode]:##For each neighboring node nNeighborNode of the current node nCurNode\n",
    "                # Get edge percantile weight\n",
    "                dMassPercentageToMove = gGraph[nCurNode][nNeighborNode]['weight'] / dWeightSum\n",
    "                \n",
    "                ##represents the percentage of the total weight (dWeightSum) that the edge weight from nCurNode to nNeighborNode contributes\n",
    "                try:\n",
    "                    # Assign part of the weight to the neighbor\n",
    "                    dMassToMove = (1.0 - dPreservationPercent) * gGraph.nodes[nCurNode][\n",
    "                        'weight'] * dMassPercentageToMove\n",
    "                    \n",
    "                    ## calculates the amount of mass to transfer from the current node (nCurNode) to its neighboring node (nNeighborNode). \n",
    "                    ## It considers the preservation percentage (dPreservationPercent), ensuring that only a portion of the mass is transferred\n",
    "                    \n",
    "                    # Work with absolute numbers, if requested\n",
    "                    \n",
    "                    ##bAbsoluteMass is True or False, the mass is added to or subtracted from the neighboring node's weight. If the mass transfer is negative (i.e., bAbsoluteMassisFalse), it means that the neighboring node loses mass, and if it's positive, it gains mass\n",
    "                    if bAbsoluteMass:##If it's True, it works with the absolute values of mass\n",
    "                        gGraph.nodes[nNeighborNode]['weight'] = abs(gGraph.nodes[nNeighborNode]['weight']) + abs(\n",
    "                            dMassToMove)\n",
    "                    else:\n",
    "                        ##if bAbsoluteMass is False, the mass is added to or subtracted from the neighboring node's weight. If the mass transfer is negative (i.e., bAbsoluteMassisFalse), it means that the neighboring node loses mass, and if it's positive, it gains mass\n",
    "                        gGraph.nodes[nNeighborNode]['weight'] += dMassToMove\n",
    "                ## If the neighboring node (nNeighborNode) has no weight assigned (KeyError), a warning message is displayed, and the weight is assigned a value of 0.\n",
    "                except KeyError:\n",
    "                    message(\"Warning: node %s has no weight assigned. Assigning 0.\" % (str(nCurNode)))\n",
    "                    gGraph.nodes[nNeighborNode]['weight'] = 0\n",
    "\n",
    "            # Reduce my weight equivalently\n",
    "            gGraph.nodes[nCurNode]['weight'] *= dPreservationPercent\n",
    "            ##reduce the weight of the current node (nCurNode) by a factor of dPreservationPercent. \n",
    "            ##This reduction is part of the spreading activation process to preserve a portion of the mass within the current node while distributing the rest to its neighboring nodes\n",
    "            ##dPreservationPercent value determines how much mass is preserved, and the remainder is distributed\n",
    "    message(\"Applying spreading activation... Done.\")\n",
    "    return gGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "702bb287-dc93-496f-91d1-80749349089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testSpreadingActivation():\n",
    "    \"\"\"\n",
    "    A harmless test of graph drawing and spreading activation effect.\n",
    "    \"\"\"\n",
    "    g = nx.Graph()  # creates an empty undirected graph g\n",
    "\n",
    "    # adds edges to the graph\n",
    "    g.add_edge(1, 2, weight=0.5)\n",
    "    g.add_edge(2, 3, weight=0.5)\n",
    "    g.add_edge(3, 4, weight=0.5)\n",
    "    g.add_edge(2, 6, weight=0.2)\n",
    "    g.add_edge(5, 6, weight=0.8)\n",
    "\n",
    "    for nNode in g.nodes():\n",
    "        g.nodes[nNode]['weight'] = nNode * 10\n",
    "        # assigns a 'weight' attribute to each node based on the node's value, node 1 has a weight of 10, node 2 has a weight of 20\n",
    "    drawAndSaveGraph(g, bSave = False)\n",
    "\n",
    "    spreadingActivation(g)  # return The (inplace) updated graph\n",
    "    drawAndSaveGraph(g, bSave = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afbf120-5451-4422-9fc7-6120e1fb1bcc",
   "metadata": {},
   "source": [
    "# check for SpreadingActivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc4c64-4e05-4516-8345-39f85b38d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = nx.Graph()  # creates an empty undirected graph g\n",
    "\n",
    "# # adds edges to the graph\n",
    "# g.add_edge(1, 2, weight=0.5)\n",
    "# g.add_edge(2, 3, weight=0.5)\n",
    "# g.add_edge(3, 4, weight=0.5)\n",
    "# g.add_edge(2, 6, weight=0.2)\n",
    "# g.add_edge(5, 6, weight=0.8)\n",
    "\n",
    "# for nNode in g.nodes():\n",
    "#     g.nodes[nNode]['weight'] = nNode * 10\n",
    "# bAbsoluteMass = False\n",
    "# drawAndSaveGraph(g, bSave = False)\n",
    "# dPreservationPercent = 0.5\n",
    "# for nCurNode in g.nodes():\n",
    "#     message(\"nCurNode:\" +str(nCurNode))\n",
    "#     # message(\"nCurNode:\" +str(nCurNode))\n",
    "#     # Get max edge weight\n",
    "#     dWeights = np.asarray([g[nCurNode][nNeighborNode]['weight'] for nNeighborNode in g[nCurNode]])\n",
    "#     message(\"dWeights:\" +str(dWeights))\n",
    "#     ## collects the weights of the edges between the nCurNode and its neighboring nodes. It does this by creating a list dWeights\n",
    "#     ##p.asarray(dWeights) converts this list of edge weights into a NumPy array\n",
    "#     dWeightSum = np.sum(dWeights)\n",
    "#     message(\"dWeightSum:\" +str(dWeightSum))\n",
    "#     ##sum of the edge weights stored in the dWeights \n",
    "#     # For every neighbor\n",
    "#     for nNeighborNode in g[nCurNode]:##For each neighboring node nNeighborNode of the current node nCurNode\n",
    "#         message(\"nNeighborNode:\" +str(nNeighborNode))\n",
    "#           # Get edge percantile weight\n",
    "#         dMassPercentageToMove = g[nCurNode][nNeighborNode]['weight'] / dWeightSum\n",
    "#         message(\"dMassPercentageToMove: \"+str(dMassPercentageToMove))\n",
    "        \n",
    "#         try:\n",
    "#             # Assign part of the weight to the neighbor\n",
    "#             dMassToMove = (1.0 - dPreservationPercent) * g.nodes[nCurNode]['weight'] * dMassPercentageToMove\n",
    "#             message(\"g.nodes[nCurNode]['weight']: \"+str(g.nodes[nCurNode]['weight']))\n",
    "#             message(\"dMassToMove: \"+str(dMassToMove))\n",
    "#             ## calculates the amount of mass to transfer from the current node (nCurNode) to its neighboring node (nNeighborNode). \n",
    "#             ## It considers the preservation percentage (dPreservationPercent), ensuring that only a portion of the mass is transferred\n",
    "            \n",
    "#             # Work with absolute numbers, if requested\n",
    "            \n",
    "#             ##bAbsoluteMass is True or False, the mass is added to or subtracted from the neighboring node's weight. If the mass transfer is negative (i.e., bAbsoluteMassisFalse), it means that the neighboring node loses mass, and if it's positive, it gains mass\n",
    "#             if bAbsoluteMass:##If it's True, it works with the absolute values of mass\n",
    "#                 g.nodes[nNeighborNode]['weight'] = abs(g.nodes[nNeighborNode]['weight']) + abs(dMassToMove)\n",
    "#                 message(\"g.nodes[nNeighborNode]['weight']: \"+str(g.nodes[nNeighborNode]['weight']))\n",
    "#             else:\n",
    "#                 ##if bAbsoluteMass is False, the mass is added to or subtracted from the neighboring node's weight. If the mass transfer is negative (i.e., bAbsoluteMassisFalse), it means that the neighboring node loses mass, and if it's positive, it gains mass\n",
    "#                 g.nodes[nNeighborNode]['weight'] += dMassToMove\n",
    "#         ## If the neighboring node (nNeighborNode) has no weight assigned (KeyError), a warning message is displayed, and the weight is assigned a value of 0.\n",
    "#         except KeyError:\n",
    "#             message(\"Warning: node %s has no weight assigned. Assigning 0.\" % (str(nCurNode)))\n",
    "#             g.nodes[nNeighborNode]['weight'] = 0\n",
    "#     g.nodes[nCurNode]['weight'] *= dPreservationPercent\n",
    "# drawAndSaveGraph(g, bSave = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
